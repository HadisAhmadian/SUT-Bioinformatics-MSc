{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrSIAHfR7g_H"
   },
   "source": [
    "<div dir=\"rtl\" align=\"center\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <font face=\"IranNastaliq\" size=5>\n",
    "      Ø¨Ù‡ Ù†Ø§Ù… Ø®Ø¯Ø§\n",
    "    </font>\n",
    "    <br>\n",
    "    <font size=3>\n",
    "      Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡ ØµÙ†Ø¹ØªÛŒ Ø´Ø±ÛŒÙ - Ø¯Ø§Ù†Ø´Ú©Ø¯Ù‡ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±\n",
    "    </font>\n",
    "    <br>\n",
    "    <font color=blue size=5>\n",
    "      Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø²Ø¨Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ø·Ø¨ÛŒØ¹ÛŒ\n",
    "    </font>\n",
    "    <br>\n",
    "    <hr/>\n",
    "    <font color=red size=6>\n",
    "      ØªÙ…Ø±ÛŒÙ† Ø¯ÙˆÙ…: ØªØ´Ø®ÛŒØµ Ú©Ù„Ù…Ø§Øª ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ\n",
    "    </font>\n",
    "    <br>\n",
    "      Ø­Ø¯ÛŒØ« Ø§Ø­Ù…Ø¯ÛŒØ§Ù†ØŒ Ø³ÛŒÙ†Ø§ Ø¹Ø¨Ø¯ÙˆØ³ØŒ Ù…Ù‡Ø³Ø§ ÛŒØ²Ø¯Ø§Ù†ÛŒ\n",
    "    <hr>\n",
    "<br>\n",
    "  <div align=\"right\">\n",
    "\n",
    "  <div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgQQ7gWv-XYV"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h1>ØªØ¹Ø±ÛŒÙ Ù…Ø³Ø¦Ù„Ù‡</h1>\n",
    "<div>Ù…Ø§ Ø¯Ø± Ø§ÛŒÙ† ØªÙ…Ø±ÛŒÙ† Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒÙ… Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙˆØ±ÙˆØ¯ÛŒ ÛŒÚ© Ù…ØªÙ† Ùˆ ÛŒÚ© Ù„ÛŒØ³Øª Ø§Ø² Ú©Ù„Ù…Ø§Øª ØºÛŒØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø¨Ú¯ÛŒØ±ÛŒÙ… Ùˆ ØªØ´Ø®ÛŒØµ Ø¯Ù‡ÛŒÙ… Ú©Ù‡ Ø¢ÛŒØ§ Ø¯Ø± Ø§ÛŒÙ† Ù…ØªÙ†ØŒ Ø§ÛŒÙ† Ú©Ù„Ù…Ø§Øª ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ ÛŒØ§ Ø®ÛŒØ± Ùˆ Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ†Ø¯ØŒ Ø¨Ø§Ø²Ù‡â€ŒÛŒ Ù‡Ø± Ú©Ù„Ù…Ù‡â€ŒÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø±Ø§ Ù†ÛŒØ² Ù…Ø´Ø®Øµ Ú©Ù†ÛŒÙ….</div>\n",
    "\n",
    "<h2>Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ</h2>\n",
    "<div> ÛŒÚ© Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø¨Ù‡ Ù†Ø§Ù… DetectIlligalWords ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ ØªØ§Ø¨Ø¹ÛŒ Ø¨Ù‡ Ù†Ø§Ù… runØŒ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Ø¢Ù† Ø§Ø³Øª. Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø¨ØªØ¯Ø§ ØªØ§Ø¨Ø¹ preprocess Ùˆ Ø³Ù¾Ø³ ØªØ§Ø¨Ø¹ process Ø±Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.\n",
    "\n",
    "<h4> ØªØ§Ø¨Ø¹ preprocess </h4>\n",
    "Ø¨Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ ØªØ§Ø¨Ø¹ run Ø§Ø¨ØªØ¯Ø§ ØªØ§Ø¨Ø¹ preprocess ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n",
    "Ø¯Ø± ØªØ§Ø¨Ø¹ preprocess Ø§Ø¨ØªØ¯Ø§ normalization Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯Ù‡ Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…ØªÙ† Ø¨Ù‡ Ù…ØªÙ† ØªØ¹Ø±ÛŒÙ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯. Ø³Ù¾Ø³ Ø±ÙˆÛŒ Ø§ÛŒÙ† Ù…ØªÙ† tokenization Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ ØªØ§Ø¨Ø¹ preprocess Ù„ÛŒØ³ØªÛŒ Ø§Ø² tokenÙ‡Ø§ Ø®ÙˆØ§Ù‡Ø¯ Ø¨ÙˆØ¯.\n",
    "\n",
    "Ù†Ú©ØªÙ‡: ØªØ¨Ø¯ÛŒÙ„ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¨Ù‡ Ù…ØªÙ† Ø¨Ù‡ Ø§ÛŒÙ† Ø¯Ù„ÛŒÙ„ Ø§Ø³Øª Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø­Ø§Ù„ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ ğŸ˜ØªØ±Ø´Ú©Ù† Ø§ÛŒØ¬Ø§Ø¯ Ø´ÙˆØ¯ Ú©Ù‡ Ø¯Ø± Ø¢Ù† Ú©Ù„Ù…Ù‡ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ ÙÛŒÙ„ØªØ±Ø´Ú©Ù† Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ ÛŒÚ© Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¨Ù‡ Ù…ØªÙ†ØŒ Ø§Ø² Ø¯ÛŒØªØ§Ø³ØªÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù‡Ø± Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¹Ø¨Ø§Ø±ØªÛŒ Ú©Ù‡ Ø¢Ù† Ø±Ø§ ØªÙˆØµÛŒÙ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ map Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "\n",
    "<h4> ØªØ§Ø¨Ø¹ process </h4>\n",
    "Ø¯Ø± Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø¨ØªØ¯Ø§ ØªØ§Ø¨Ø¹ detect Ø±ÙˆÛŒ Ø®ÙˆØ¯ tokenÙ‡Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ø´Ø¯Ù‡ Ùˆ Ø³Ù¾Ø³ ÛŒÚ© Ø¨Ø§Ø± Ù‡Ù… Ø±ÙˆÛŒ lemmaÛŒ tokenÙ‡Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯. Ú†Ø±Ø§ Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡ Ø·ÙˆØ± Ù…Ø«Ø§Ù„ Ø¨Ø¬Ø§ÛŒ Ø®ÙˆØ¯ Ú©Ù„Ù…Ù‡â€ŒÛŒ ØªÙÙ†Ú¯ Ø¯Ø± Ù…ØªÙ† Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ… ØªÙÙ†Ú¯â€ŒÙ‡Ø§.\n",
    "ØªØ§Ø¨Ø¹ detect ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø² ØªÙˆØ§Ø¨Ø¹ Ø±Ø§ ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø¢Ù†â€ŒÙ‡Ø§ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„ÙÛŒ Ø§Ø² Ø¨Ù‡ Ú©Ø§Ø± Ø¨Ø±Ø¯Ù† Ú©Ù„Ù…Ù‡â€ŒÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ Ú©Ù‡ Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² Ø§ÛŒÙ† ØªÙˆØ§Ø¨Ø¹ Ùˆ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒÛŒ Ø§Ø² Ù…ÙˆØ§Ø±Ø¯ÛŒ Ú©Ù‡ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ù†Ø¯ØŒ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJpCFRbLC_1E"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<hr/>\n",
    "Ø¯Ø± Ø³Ù„ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø¨Ø¹Ø¯ÛŒØŒ Ù…Ø§Ú˜ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ requirements.txt Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ØŒ Ø§Ø¨ØªØ¯Ø§ Ù†ØµØ¨ Ùˆ Ø³Ù¾Ø³ import Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTegHxYx4wrM"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import Text\n",
    "!touch /content/requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udG-vaK3BVlQ",
    "outputId": "de360d90-24e6-4d88-82a7-c84cc4c70591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/requirments.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/requirments.txt\n",
    "\n",
    "hazm\n",
    "num2fawords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh4KOe7x5Ctn",
    "outputId": "bf2d531a-3571-4b5c-e34b-41ae088c6de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: hazm in /usr/local/lib/python3.9/dist-packages (from -r /content/requirments.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: num2fawords in /usr/local/lib/python3.9/dist-packages (from -r /content/requirments.txt (line 3)) (1.1)\n",
      "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from hazm->-r /content/requirments.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.9/dist-packages (from hazm->-r /content/requirments.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk==3.3->hazm->-r /content/requirments.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /content/requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXebGbB8cRgz"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5gNTwL6D0EC"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h3> ØªÙˆØ§Ø¨Ø¹ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´Ø¯Ù‡ </h3>\n",
    "<h4> ØªØ§Ø¨Ø¹ is_meaningful_word </h4>\n",
    "Ø¨Ù‡ Ù…Ù†Ø¸ÙˆØ± Ø§ÛŒÙ†Ú©Ù‡ Ø§Ø´ØªØ¨Ø§Ù‡ÛŒ Ø¯Ø± ØªØ´Ø®ÛŒØµ Ú©Ù„Ù…Ø§Øª ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù¾ÛŒØ´ Ù†ÛŒØ§ÛŒØ¯ Ùˆ Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú©Ù„Ù…Ù‡â€ŒØ§ÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ú©Ù‡ Ø¯Ø± Ø¨ÛŒÙ† Ø­Ø±ÙˆÙ Ø¢Ù† Ú©Ø§Ø±Ø§Ú©ØªØ±â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø®Ø§Øµ Ø¨Ù‡ Ú©Ø§Ø± Ø±ÙØªÙ‡ Ø§Ø³Øª ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù†Ø´ÙˆØ¯ØŒ Ù¾Ø³ Ø§Ø² ØªØ´Ø®ÛŒØµ Ø­Ø§Ù„Øª Ø§Ø¨ØªØ¯Ø§ÛŒÛŒ Ú©Ù„Ù…Ø§Øª ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ (ÛŒØ¹Ù†ÛŒ Ø¨Ù‡ Ø´Ú©Ù„ Ù…Ø³ØªÙ‚ÛŒÙ… Ø¨Ù‡ Ú©Ø§Ø± Ø±ÙØªÙ† Ø¢Ù†â€ŒÙ‡Ø§)ØŒ tokenÙ‡Ø§ÛŒ Ù…Ø¹Ù†ÛŒ Ø¯Ø§Ø± Ø§Ø² Ù„ÛŒØ³Øª tokenÙ‡Ø§ Ø­Ø°Ù Ø®ÙˆØ§Ù‡Ù†Ø¯ Ø´Ø¯.\n",
    "<br>\n",
    "Ø¨Ù‡ Ø§ÛŒÙ† Ù…Ù†Ø¸ÙˆØ± ÙØ§ÛŒÙ„ÛŒ Ø¨Ù‡ Ù†Ø§Ù… persian_words.txt Ø¯Ø± Ù¾ÙˆØ´Ù‡â€ŒÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ú©Ù„Ù…Ø§Øª Ù…Ø¹Ù†Ø§Ø¯Ø§Ø± ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª Ùˆ Ø¨Ø±Ø§ÛŒ Ú†Ú© Ú©Ø±Ø¯Ù† Ù…Ø¹Ù†ÛŒâ€ŒØ¯Ø§Ø± Ø¨ÙˆØ¯Ù† ÛŒØ§ Ù†Ø¨ÙˆØ¯Ù† ÛŒÚ© Ú©Ù„Ù…Ù‡ØŒ Ú†Ú© Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú©Ù‡ Ø¢ÛŒØ§ Ú©Ù„Ù…Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ù„ÛŒØ³Øª Ù‡Ø³Øª ÛŒØ§ Ø®ÛŒØ±.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJBk0xiIaNQG",
    "outputId": "68fad3c1-afff-4612-a49a-a375d3f2ca91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FGMH4pzNmN3x8WE4vGpd03F95O4pITcL\n",
      "To: /content/persian_words.txt\n",
      "100% 25.8M/25.8M [00:00<00:00, 142MB/s] \n"
     ]
    }
   ],
   "source": [
    "# read persian words from text file and convert to a list\n",
    "%cd /content\n",
    "!gdown 1FGMH4pzNmN3x8WE4vGpd03F95O4pITcL\n",
    "\n",
    "with open('/content/persian_words.txt', 'r', encoding=\"utf8\") as f:\n",
    "    persian_words = [line.strip() for line in f.read().splitlines()]\n",
    "\n",
    "# cheks whether the token is meaningful or not\n",
    "def is_meaningful_word(token):\n",
    "  if token in persian_words:\n",
    "      return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HUda75tE-nt"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªØ§Ø¨Ø¹ detect_with_star </h4>\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ØªØ§Ø¨Ø¹ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø­Ø§Ù„Ø§ØªÛŒ Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¯Ø± Ø¢Ù† ÛŒÚ© ÛŒØ§ ØªØ¹Ø¯Ø§Ø¯ÛŒ Ø§Ø² Ø­Ø±ÙˆÙ Ú©Ù„Ù…Ù‡â€ŒÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø¨Ø§ * Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„ Ø¨Ø¬Ø§ÛŒ ØªÙÙ†Ú¯ Ø­Ø§Ù„ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ Øª**Ú¯ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2G6e58pE1rk"
   },
   "outputs": [],
   "source": [
    "# examples: \"ØªÙ*Ú¯ Øª**Ú¯ *Ù*Ú¯ Ù†Ù*Ú¯\" for \"ØªÙÙ†Ú¯\"\n",
    "def detect_with_star(token, illegal_words):\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  To check asterisk occurrence in the word.\n",
    "  Illegal and input words should have the same length\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  if token == '.':\n",
    "    return None\n",
    "\n",
    "  for illegal_word in illegal_words:\n",
    "\n",
    "    if len(token) < len(illegal_word):\n",
    "      return None\n",
    "\n",
    "    pattern = re.compile(token.replace(\"*\", \"[[\\u0600-\\u06FF]|Ú¯|Ú†|Ù¾|Ú˜]\"), re.IGNORECASE)\n",
    "    match = re.search(pattern, illegal_word)\n",
    "    if match:\n",
    "      return illegal_word\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "# detect_with_star('Ù†Ú¯', ['ØªÙÙ†Ú¯'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSlPM2EXh5V6"
   },
   "outputs": [],
   "source": [
    "def all_formats_a(word,all,indx): \n",
    "  all.add(word)\n",
    "  flag=True\n",
    "  for i in range(indx,len(word)):\n",
    "    if word[i]==\"a\" or word[i]==\"A\":\n",
    "      flag=False\n",
    "      all_formats_a(word,all,i+1)\n",
    "      all_formats_a(word[:i]+word[i+1:],all,i)\n",
    "  return all\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq8hVfp9VK7c"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªÙˆØ§Ø¨Ø¹  all_formats_o , all_formats_a </h4>\n",
    "Ø§ÛŒÙ† ØªÙˆØ§Ø¨Ø¹ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù„Ù…Ø§Øª ÙÛŒÙ†Ú¯Ù„ÛŒØ´ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯Ù‡Ù†Ø¯. Ø¯Ø± ÙØ§Ø±Ø³ÛŒ Ù…Ø§ Ø§Ø¹Ø±Ø§Ø¨ Ú©Ù„Ù…Ø§Øª Ø±Ø§ Ù†Ù…ÛŒÙ†ÙˆÛŒØ³ÛŒÙ… Ø§Ù…Ø§ Ø¯Ø± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ø§ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ o Ùˆ a Ùˆ e Ø§Ø¹Ø±Ø§Ø¨ Ù‡Ø§ Ø±Ø§ Ù†ÛŒØ² Ù…ÛŒÙ†ÙˆÛŒØ³ÛŒÙ…ØŒ Ø­Ø§Ù„ Ø¢Ù†Ú©Ù‡ Ø¨Ø¹Ø¶ÛŒ Ø§Ø² Ø§ÛŒÙ† Ú©Ø§Ø±Ø§Ú©ØªØ± Ù‡Ø§ (Ù…Ø«Ù„ a) Ù…ÛŒØªÙˆØ§Ù†Ø¯ Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø§Ø¹Ø±Ø§Ø¨ Ø¨Ù‡ Ú©Ø§Ø± Ø±ÙˆØ¯ Ùˆ Ù‡Ù… Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø­Ø±Ù Ø§Ù„Ù Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯. Ù¾Ø³ Ø¨Ø§ÛŒØ¯ ÛŒÚ© Ø³Ø±ÛŒ Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆÛŒ Ú©Ù„Ù…Ù‡ ÛŒ ÙÛŒÙ†Ú¯Ù„ÛŒØ´ Ø§Ù†Ø¬Ø§Ù… Ø¯Ø§Ø¯ ØªØ§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ø§ÛŒÙ† Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ Ø¨Ø§ Ú©Ø§Ø±Ø§Ú©ØªØ± Ù‡Ø§ÛŒ Ú©Ù„Ù…Ù‡ ÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø´ÙˆÙ†Ø¯ Ùˆ Ø­Ø§Ù„Ø§Øª Ù…Ø®ØªÙ„Ù Ø¢Ù†Ù‡Ø§ (Ø§ÛŒÙ†Ú©Ù‡ Ø§Ø¹Ø±Ø§Ø¨ Ø¨ÙˆØ¯Ù†Ø¯ ÛŒØ§ Ù…Ø¹Ø§Ø¯Ù„ ÛŒÚ© Ù…ØµÙˆØª Ø¢Ù…Ø¯Ù‡ Ø§Ù†Ø¯) Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ø´ÙˆØ¯\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2U7YcNmLb1K"
   },
   "outputs": [],
   "source": [
    "def all_formats_o(word,all,indx):\n",
    "  all.add(word)\n",
    "  for i in range(indx,len(word)):\n",
    "    if word[i]==\"o\" or word[i]==\"O\":\n",
    "      all_formats_o(word[:i]+\"v\"+word[i+1:],all,i+1)\n",
    "      all_formats_o(word[:i]+word[i+1:],all,i)\n",
    "\n",
    "  return all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIsyVNcyWC2F"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªÙˆØ§Ø¨Ø¹  ENG_to_format , PRS_to_format </h4>\n",
    "Ø§ÛŒÙ† ØªÙˆØ§Ø¨Ø¹ Ù†ÛŒØ² Ù¾ÛŒØ´ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ú©Ù„Ù…Ø§Øª ÙÛŒÙ†Ú¯Ù„ÛŒØ´ Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯Ù‡Ù†Ø¯. Ù‡Ù…Ø§Ù†Ø·ÙˆØ± Ú©Ù‡ Ú¯ÙØªÙ‡ Ø´Ø¯ Ú©Ù„Ù…Ø§Øª ÙØ§Ø±Ø³ÛŒ Ùˆ ÙÛŒÙ†Ú¯Ù„ÛŒØ´ Ø¯Ø± Ù†Ú¯Ø§Ø±Ø´ Ø®ÙˆØ¯ ØªÙØ§ÙˆØª Ù‡Ø§ÛŒÛŒ Ø¯Ø§Ø±Ù†Ø¯ Ú©Ù‡ Ù…Ù‚Ø§ÛŒØ³Ù‡ ÛŒ Ø¢Ù†Ù‡Ø§ Ø¨Ù‡ Ø±Ø§Ø­ØªÛŒ Ùˆ Ø¨Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ ÛŒ Ø­Ø±Ù Ø¨Ù‡ Ø­Ø±Ù Ù…Ù…Ú©Ù† Ù†ÛŒØ³Øª Ùˆ Ø¨Ø§ÛŒØ¯ ØªÙØ§ÙˆØª Ù‡Ø§ÛŒ Ø¢Ù† Ù‡Ø§ Ø±Ø§ Ø¯Ø±Ù†Ø¸Ø± Ú¯Ø±ÙØª. Ø­Ø§Ù„Ø§Øª Ø²ÛŒØ§Ø¯ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø¯Ø± ÙÛŒÙ†Ú¯Ù„ÛŒØ´ Ù†ÙˆØ´ØªÙ‡ Ù…ÛŒØ´ÙˆØ¯ ÙˆÙ„ÛŒ Ø¯Ø± ÙØ§Ø±Ø³ÛŒ Ù†Ù‡ (Ù…Ø«Ù„ Ø§Ø¹Ø±Ø§Ø¨) Ùˆ Ø­Ø§Ù„Ø§Øª Ø²ÛŒØ§Ø¯ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ Ú©Ù‡ Ø¯Ø± ÙØ§Ø±Ø³ÛŒ Ù†ÙˆØ´ØªÙ‡ Ù…ÛŒØ´ÙˆØ¯ ÙˆÙ„ÛŒ Ø¯Ø± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù†Ù‡! Ù…Ø§Ù†Ù†Ø¯ Ø¹ Ø§ÙˆÙ„ Ú©Ù„Ù…Ù‡ (Ù…Ø«Ù„Ø§ Ø¹Ø§Ù…Ù„ =Ùamel)\n",
    "Ø¨Ù‡ Ù‡Ù…ÛŒÙ† Ø¹Ù„Øª Ø¨Ø§ÛŒØ¯ Ú©Ù„Ù…Ù‡ ÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ú©Ù‡ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ùˆ Ú©Ù„Ù…Ù‡ ÛŒ ÙÛŒÙ†Ú¯Ù„ÛŒØ´ Ù‡Ø±Ø¯Ùˆ Ø¨Ù‡ ÛŒÚ© ÙØ±Ù…ØªÛŒ Ø¨Ø±Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ Ú©Ù‡ Ù‚Ø§Ø¨Ù„ÛŒØª Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§Ù‡Ù… Ø±Ø§ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯. ØªÙˆØ§Ø¨Ø¹ Ø²ÛŒØ± Ø§ÛŒÙ† Ú©Ø§Ø± Ø±Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯Ù‡Ù†Ø¯.\n",
    "Ù…Ø«Ø§Ù„ Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ Ø§Ø² ÛŒÚ©Ø³Ø§Ù† Ø³Ø§Ø²ÛŒ ÙØ±Ù…Øª Ú©Ù‡ ØªÙˆØ§Ø¨Ø¹ Ø²ÛŒØ± Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒØ¯Ù‡Ø¯  Ù…Ø§Ù†Ù†Ø¯ Ù…ØµÙˆØª Ù‡Ø§ÛŒ Ø²ÛŒØ§Ø¯ÛŒ Ú©Ù‡ Ù…Ø¹Ø§Ø¯Ù„ ÛŒÚ© ØµØ¯Ø§ Ù‡Ø³ØªÙ†Ø¯ Ù…Ø«Ù„ Ø¶ Ø² Ø¸ Ø° ÙˆÙ„ÛŒ Ø¯Ø± Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù‡Ù…Ú¯ÛŒ z Ù†ÙˆØ´ØªÙ‡ Ù…ÛŒØ´ÙˆØ¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMcfis-Ye5SB"
   },
   "outputs": [],
   "source": [
    "def ENG_to_format(word):\n",
    "\n",
    "  if word[0]==\"e\" or word[0]==\"o\":\n",
    "    word=\"@\"+word[1:]\n",
    "  if word[0]==\"a\" or word[0]==\"i\" or word[0]==\"o\":\n",
    "    word=\"@\"+word\n",
    "  if word[-1]==\"e\":\n",
    "    word=word[:len(word)-1]+\"h\"\n",
    "\n",
    "  word=word.replace(\"oo\",\"v\")\n",
    "  \n",
    "  word=word.replace(\"e\",\"\")\n",
    "  word=word.replace(\"u\",\"v\")\n",
    "  word=word.replace(\"i\",\"y\")\n",
    "  word=word.replace(\"ee\",\"y\")\n",
    "\n",
    "  all=set([word])\n",
    "  all=all_formats_a(word,all,0)\n",
    "  all_list=list(all)\n",
    "\n",
    "  for x in all_list:\n",
    "    all_formats_o(x,all,0)\n",
    "\n",
    "  all_list=list(all)\n",
    "\n",
    "\n",
    "  res=[]\n",
    "\n",
    "  for x in all_list:\n",
    "    res.append(x.replace(\"o\",\"\"))\n",
    "  return(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nm3cCt8RmtNa"
   },
   "outputs": [],
   "source": [
    "def PRS_to_format(word):\n",
    "\n",
    "  res=\"\"\n",
    "  if word[0]==\"Ø¹\" or word[0]==\"Ø¢\" or word[0]==\"Ø§\":\n",
    "    res+=\"@\"\n",
    "\n",
    "  for w in word:\n",
    "    if w==\"Ø§\":\n",
    "      res+=\"a\"\n",
    "    elif w==\"Ø¨\":\n",
    "      res+=\"b\"\n",
    "    elif w==\"Ù¾\":\n",
    "      res+=\"p\"\n",
    "    elif w==\"Øª\" or w==\"Ø·\":\n",
    "      res+=\"t\"\n",
    "    elif w==\"Ø«\" or w==\"Øµ\" or w==\"Ø³\":\n",
    "      res+=\"s\"\n",
    "    elif w==\"Ø¬\":\n",
    "      res+=\"j\"\n",
    "    elif w==\"Ú†\":\n",
    "      res+=\"ch\"\n",
    "    elif w==\"Ø­\":\n",
    "      res+=\"h\"\n",
    "    elif w==\"Ø®\":\n",
    "      res+=\"kh\"\n",
    "    elif w==\"Ø¯\":\n",
    "      res+=\"d\"\n",
    "    elif w==\"Ø°\"or w==\"Ø¶\"or w==\"Ø²\"or w==\"Ø¸\":\n",
    "      res+=\"z\"\n",
    "    elif w==\"Ø±\":\n",
    "      res+=\"r\"\n",
    "    elif w==\"Ú˜\":\n",
    "      res+=\"zh\"\n",
    "    elif w==\"Ø´\":\n",
    "      res+=\"sh\"\n",
    "    elif w==\"Ù‚\"or w==\"Øº\":\n",
    "      res+=\"q\"\n",
    "    elif w==\"Ù\":\n",
    "      res+=\"f\"\n",
    "    elif w==\"Ú©\":\n",
    "      res+=\"k\"\n",
    "    elif w==\"Ú¯\":\n",
    "      res+=\"g\"\n",
    "    elif w==\"Ù„\":\n",
    "      res+=\"l\"\n",
    "    elif w==\"Ù…\":\n",
    "      res+=\"m\"\n",
    "    elif w==\"Ù†\":\n",
    "      res+=\"n\"\n",
    "    elif w==\"Ùˆ\":\n",
    "      res+=\"v\"\n",
    "    elif w==\"Ù‡\" or w==\"Ø­\":\n",
    "      res+=\"h\"\n",
    "    elif w==\"ÛŒ\":\n",
    "      res+=\"y\"\n",
    "  return res\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQabcBDoesCF"
   },
   "outputs": [],
   "source": [
    "def check_FiEnglish(word,illegal):\n",
    "  if PRS_to_format(illegal) in list(word):\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRaPkY0MGU_R"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªØ§Ø¨Ø¹ detect_fingilish </h4>\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù…ÙˆØ§Ø±Ø¯ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù† Ú©Ù„Ù…Ù‡â€ŒÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø¨Ù‡ Ø´Ú©Ù„ ÙÛŒÙ†Ú¯ÛŒÙ„Ø´ Ù†ÙˆØ´ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ Ø±Ø§ ØªØ´Ø®ÛŒØµ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯. Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„ Ø§Ú¯Ø± Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ù‡â€ŒÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ ÙÛŒÙ„ØªØ±Ø´Ú©Ù† Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…: filtershekan\n",
    "Ø¯Ø± Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø¨ØªØ¯Ø§ Ú©Ù„Ù…Ù‡ ÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ùˆ ØªÙˆÚ©Ù† Ù‡Ø±Ø¯Ùˆ Ø¨Ù‡ ÙØ±Ù…Øª Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¨Ø±Ø¯Ù‡ Ù…ÛŒØ´ÙˆÙ†Ø¯ Ùˆ Ø¨Ø§ check_finglish Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ÛŒØ´ÙˆØ¯ Ú©Ù‡ Ø¢ÛŒØ§ ØªÙˆÚ©Ù† ÙÛŒÙ†Ú¯Ù„ÛŒØ´ Ù…Ø¹Ø§Ø¯Ù„ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø¨ÙˆØ¯Ù‡ Ø§Ø³Øª ÛŒØ§ Ù†Ù‡.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRgVGFg3iaxN"
   },
   "outputs": [],
   "source": [
    "def detect_fingilish(token, illegal_words):\n",
    "  word=ENG_to_format(token)\n",
    "  for ill in illegal_words:\n",
    "    if check_FiEnglish(word,ill):\n",
    "      return ill\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o1XR0QYF5jy"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªØ§Ø¨Ø¹ detect_typo </h4>\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ØºÙ„Ø· ØªØ§ÛŒÙ¾ÛŒ Ø¨Ø§ Ø§ÛŒÙ† Ù‚Ø§Ù†ÙˆÙ† Ø§Ø³Øª Ú©Ù‡ ÛŒÚ© Ø­Ø±Ù Ø§Ø² Ø­Ø±ÙˆÙ Ú©Ù„Ù…Ù‡ ÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø¨Ø§ ÛŒÚ© Ú©Ø§Ø±Ø§Ú©ØªØ± Ø§Ø´ØªØ¨Ø§Ù‡ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ ÛŒØ§ ÛŒÚ© Ø­Ø±Ù Ù¾Ø§Ú© Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. Ù…Ø«Ù„ ÙÛŒÙ„ØªØ±Ø´Ú©Ù† Ùˆ ÙÛŒÙ„ØªØ±Ø´Ú©Ù…\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxc9fGANOUqJ"
   },
   "outputs": [],
   "source": [
    "def detect_typo(token, illegal_words):\n",
    "  for ill in illegal_words:\n",
    "    if len(token)==len(ill) and sum(c1 != c2 for c1, c2 in zip(token, ill))==1:\n",
    "      return ill\n",
    "    elif len(token)==len(ill)-1:\n",
    "      if sum(c1 != c2 for c1, c2 in zip(\"@\"+token, ill))==1 or sum(c1 != c2 for c1, c2 in zip(token+\"@\", ill))==1:\n",
    "          return ill\n",
    "      for i in range(len(token)-1):\n",
    "        if sum(c1 != c2 for c1, c2 in zip(token[:i+1]+\"@\"+token[i+1:], ill))==1:\n",
    "          return ill\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYlVLQ9QYbtk"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªØ§Ø¨Ø¹ make_regex </h4>\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ú©Ù„Ù…Ù‡ ÛŒ ØºÛŒØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø±Ø§ Ø¯Ø±ÛŒØ§ÙØª Ú©Ø±Ø¯Ù‡ Ùˆ ÛŒÚ© regular expression Ø¨Ø±Ø§ÛŒ Ø¢Ù† Ù…ÛŒÙ†ÙˆÛŒØ³Ø¯. Ø§ÛŒÙ† RE Ù‚Ø±Ø§Ø± Ø§Ø³Øª Ø¯Ø± Ø§Ø¯Ø§Ù…Ù‡ Ø­Ø§Ù„Øª Ù‡Ø§ÛŒ Ù…Ø®ØªÙ„ÙÛŒ Ø§Ø² Ù†ÙˆØ´ØªÙ† Ú©Ù„Ù…Ù‡ ÛŒ ØºÛŒØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø±Ø§ Ø´Ø§Ù…Ù„ Ø´ÙˆØ¯ Ú©Ù‡ Ø¨Ù‡ Ø´Ø±Ø­ Ø²ÛŒØ± Ù‡Ø³ØªÙ†Ø¯\n",
    "</br>\n",
    "1- Ø­Ø§Ù„Ø§Øª Ù…ØµÙˆØª Ù‡Ø§ÛŒ Ù‡Ù… ØµØ¯Ø§ (Ø¯Ø±ÙˆØ§Ù‚Ø¹ ØºÙ„Ø· Ø§Ù…Ù„Ø§ÛŒÛŒ) : ØµÛŒÚ¯Ø§Ø± ÛŒØ§ Ø«ÛŒÚ¯Ø§Ø± Ø¨Ù‡ Ø¬Ø§ÛŒ Ø³ÛŒÚ¯Ø§Ø±\n",
    "</br>\n",
    "2- Ø­Ø§Ù„Ø§Øª Ø­Ø±ÙˆÙ Ù‡Ù… ØµØ¯Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ Ø¬Ø§ÛŒ ØµØ¯Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ù…Ø«Ù„ sÛŒÚ¯Ø§Ø± Ø¨Ù‡ Ø¬Ø§ÛŒ Ø³ÛŒÚ¯Ø§Ø±</br>\n",
    "3- Ø­Ø§Ù„Ø§Øª Ø­Ø±ÙˆÙ ØºÛŒØ± ÙØ§Ø±Ø³ÛŒ Ø¨ÛŒÙ† Ø­Ø±ÙˆÙ Ú©Ù„Ù…Ù‡ Ù…Ø§Ù†Ù†Ø¯ Ø³%ÛŒÚ¯Ø§$$Ø±</br>\n",
    "4- Ù‡Ø± ØªØ±Ú©ÛŒØ¨ÛŒ Ø§Ø² Ø­Ø§Ù„Ø§Øª ÙÙˆÙ‚ØŒ Ù…Ø«Ù„Ø§ ØµÛŒ&g#Ø§Ø±</br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq6hSsKGesQC"
   },
   "outputs": [],
   "source": [
    "def make_regex(word,gp):\n",
    "  a=\"[a,A,Ø§,Ø¢]\"\n",
    "  b=\"[b,B,Ø¨]\"\n",
    "  p=\"[p,P,Ù¾]\"\n",
    "  t=\"[t,T,Ø·,Øª]\"\n",
    "  s=\"[s,S,c,S,Ø³,Øµ,Ø«]\"\n",
    "  j=\"[j,J,g,G,Ø¬]\"\n",
    "  ch=\"[ch|CH|cH|Ch|Ú†]\"\n",
    "  h=\"[h,H,Ø­,Ù‡]\"\n",
    "  kh=\"[kh|kH|KH|Kh|Ø®]\"\n",
    "  d=\"[d|D|Ø¯]\"\n",
    "  z=\"[z|Z|Ø²|Ø¶|Ø°|Ø¸]\"\n",
    "  r=\"[r|R|Ø±]\"\n",
    "  zh=\"[zH|ZH|Zh|zh|Ú˜|j]\"\n",
    "  q=\"[q|Q|Ù‚|Øº]\"\n",
    "  g=\"[g|G|Ú¯]\"\n",
    "  m=\"[m|M|Ù…]\"\n",
    "  v=\"[v|W|w|V|u|U|Ùˆ|o|O|oo|Oo|oO]\"\n",
    "  y=\"[i|I|Y|y|ÛŒ]\"\n",
    "  f=\"[f,F,Ù]\"\n",
    "  l=\"[l,L,Ù„]\"\n",
    "  t=\"[t,T,Øª,Ø·]\"\n",
    "  sh=\"[sh|Ø´|Sh|SH|sH]\"\n",
    "  k=\"[k,K,c,C,Ú©]\"\n",
    "  n=\"[n,N,Ù†]\"\n",
    "\n",
    "\n",
    "  res=\"\"\n",
    "  for w in word:\n",
    "    if w==\"Ø¢\" or w==\"Ø§\":\n",
    "      res+=a+gp\n",
    "    elif w==\"Ø¨\":\n",
    "      res+=b+gp\n",
    "    elif w==\"Ù¾\":\n",
    "      res+=p+gp\n",
    "    elif w==\"Øª\" or w==\"Ø·\":\n",
    "      res+=t+gp\n",
    "    elif w==\"Ø«\" or w==\"Øµ\" or w==\"Ø³\":\n",
    "      res+=s+gp\n",
    "    elif w==\"Ø¬\":\n",
    "      res+=j+gp\n",
    "    elif w==\"Ú†\":\n",
    "      res+=ch+gp\n",
    "    elif w==\"Ø­\":\n",
    "      res+=h+gp\n",
    "    elif w==\"Ø®\":\n",
    "      res+=kh+gp\n",
    "    elif w==\"Ø¯\":\n",
    "      res+=d+gp\n",
    "    elif w==\"Ø°\"or w==\"Ø¶\"or w==\"Ø²\"or w==\"Ø¸\":\n",
    "      res+=z+gp\n",
    "    elif w==\"Ø±\":\n",
    "      res+=r+gp\n",
    "    elif w==\"Ú˜\":\n",
    "      res+=zh+gp\n",
    "    elif w==\"Ø´\":\n",
    "      res+=sh+gp\n",
    "    elif w==\"Ù‚\"or w==\"Øº\":\n",
    "      res+=q+gp\n",
    "    elif w==\"Ù\":\n",
    "      res+=f+gp\n",
    "    elif w==\"Ú©\":\n",
    "      res+=k+gp\n",
    "    elif w==\"Ú¯\":\n",
    "      res+=g+gp\n",
    "    elif w==\"Ù„\":\n",
    "      res+=l+gp\n",
    "    elif w==\"Ù…\":\n",
    "      res+=m+gp\n",
    "    elif w==\"Ù†\":\n",
    "      res+=n+gp\n",
    "    elif w==\"Ùˆ\":\n",
    "      res+=v+gp\n",
    "    elif w==\"Ù‡\" or w==\"Ø­\":\n",
    "      res+=h+gp\n",
    "    elif w==\"ÛŒ\":\n",
    "      res+=y+gp\n",
    "\n",
    "  return res[:len(res)-len(gp)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7eKbX8FE_X9"
   },
   "outputs": [],
   "source": [
    "def if_illegal_alter(txt,illegal):\n",
    "  x = re.search(illegal, txt)\n",
    "  if x:\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7mne-rEGvyi"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªØ§Ø¨Ø¹ detect_non_persian_alternative </h4>\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ ØªÙˆÚ©Ù† Ø±Ø§ Ø¨Ø§ RE Ú©Ù„Ù…Ù‡ ÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ú©Ù‡ Ø¯Ø± Ø¨Ø§Ù„Ø§ ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…ÛŒÚ©Ù†Ø¯ Ú©Ù‡ Ø§Ú¯Ø± ÛŒÚ©ÛŒ Ø§Ø² Ø­Ø§Ù„Ø§Øª ØªÙˆØ¶ÛŒØ­ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡ Ø±Ø® Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ ØªÙˆÚ©Ù† Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú©Ù„Ù…Ù‡ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø¹Ø±ÙÛŒ Ù…ÛŒÚ©Ù†Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubzhR5_qnE6w"
   },
   "outputs": [],
   "source": [
    "def detect_non_persian_alternative(token, illegal_words):\n",
    "  non_pers=\"[^Ù¾,Ú†,Ø¬,Ø­,Ø®,Ù‡,Ø¹,Øº,Ù,Ù‚,Ø«,Øµ,Ø¶,Ú¯,Ú©,Ù…,Ù†,Øª,Ø§,Ù„,Ø¨,ÛŒ,Ø³,Ø´,Ùˆ,Ø¦,Ø¯,Ø°,Ø±,Ø²,Ø·,Ø¸,Ø¢,Ø©,ÙŠ,Ú˜,Ø¤,Ø¥,Ø£,Ø¡,Û€]*\"\n",
    "  for ill in illegal_words:\n",
    "    reg=make_regex(ill,non_pers)\n",
    "    if if_illegal_alter(token,reg):\n",
    "      return ill\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xO1XcKBaOfg"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªØ§Ø¨Ø¹ detect_with_spaces </h4>\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø­Ø§Ù„Ø§ØªÛŒ Ø±Ø§ Ù‡Ù†Ø¯Ù„ Ù…ÛŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¨ÛŒÙ† Ú©Ù„Ù…Ù‡ ÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø¹Ù„Ø§ÙˆÙ‡ Ø¨Ø± Ú©Ø§Ø±Ø§Ú©ØªØ± ØºÛŒØ±ÙØ§Ø±Ø³ÛŒ ÙØ§ØµÙ„Ù‡ Ø¨Ø§Ø´Ø¯ Ù…Ø«Ù„Ø§ ÙÛŒ Ù„ØªØ± Ø´Ú©Ù†\n",
    "</br>\n",
    "Ù†Ú©ØªÙ‡1: Ø¹Ù„Øª Ø¬Ø¯Ø§ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø§Ø² RE Ù‚Ø¨Ù„ÛŒ Ø§ÛŒÙ† Ù‡Ø³Øª Ú©Ù‡ Ø¯Ø± ØªÙˆÚ©Ù† Ú©Ø±Ø¯Ù† Ø¨Ø± Ø­Ø³Ø¨ ÙØ§ØµÙ„Ù‡ Ø¨Ø®Ø´ Ù‡Ø§ÛŒ Ù…Ø®Ù„Ù Ú©Ù„Ù…Ù‡ ÛŒ Ø¬Ø¯Ø§ Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø³Ù¾ÛŒØ³ Ø§Ø² Ù‡Ù… Ø¬Ø¯Ø§ Ù…ÛŒØ´ÙˆÙ†Ø¯ Ùˆ Ø¨Ø§ÛŒØ¯ ÛŒÚ© ØªØ§Ø¨Ø¹ Ø¬Ø¯Ø§ Ø¨Ø±Ø§ÛŒØ´ ØªØ¹Ø±ÛŒÙ Ú©Ø±Ø¯\n",
    "</br>\n",
    "Ù†Ú©ØªÙ‡2: Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø­Ø§Ù„Ø§Øª Ø®Ø§Øµ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Ù…ØµÙˆØª Ù‡Ø§ Ø±Ø§ Ù‡Ù… Ù‡Ù†Ø¯Ù„ Ù…ÛŒÚ©Ù†Ø¯ Ù…Ø«Ù„ ØµÛŒ gØ§Ø±\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "id": "qMf8J1rMOol5"
   },
   "outputs": [],
   "source": [
    "def detect_with_spaces(self,text,illegal_words):\n",
    "  res=dict([])\n",
    "  for ill in illegal_words:\n",
    "    reg=make_regex(ill,\"[^Ù¾,Ú†,Ø¬,Ø­,Ø®,Ù‡,Ø¹,Øº,Ù,Ù‚,Ø«,Øµ,Ø¶,Ú¯,Ú©,Ù…,Ù†,Øª,Ø§,Ù„,Ø¨,ÛŒ,Ø³,Ø´,Ùˆ,Ø¦,Ø¯,Ø°,Ø±,Ø²,Ø·,Ø¸,Ø¢,Ø©,ÙŠ,Ú˜,Ø¤,Ø¥,Ø£,Ø¡,Û€]*\")\n",
    "    for m in re.finditer(reg, text):\n",
    "      mIdx=self.find_idx(text[m.start():m.end()],text,self.current_map)\n",
    "      if not ill in res:\n",
    "        res[ill]=set([mIdx])\n",
    "      else:\n",
    "        res[ill].add(mIdx)\n",
    "      \n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6l7t06aHAbh5"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªØ§Ø¨Ø¹ detect_with_useless_persian_letters </h4>\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø­Ø§Ù„Ø§ØªÛŒ Ø±Ø§ Ù‡Ù†Ø¯Ù„ Ù…ÛŒÚ©Ù†Ø¯ Ú©Ù‡ Ù‡Ø± ÛŒÚ© Ø§Ø² Ø­Ø±ÙˆÙ Ú©Ù„Ù…Ù‡ ØºÛŒØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒØŒ Ø¯Ø± Ø¬Ø§ÛŒ Ø®ÙˆØ¯ ØªÚ©Ø±Ø§Ø± Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯.\n",
    "Ø¨Ø¯ÛŒÙ† ØªØ±ØªÛŒØ¨ Ø§Ù…Ú©Ø§Ù† ØªØ´Ø®ÛŒØµ \"ÙÛŒÙ„Ù„Ù„Ù„Ù„ØªØ±Ø´Ú©Ú©Ú©Ú©Ú©Ù†\"\n",
    "Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ù‡ ØºÛŒØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ\n",
    "'ÙÛŒÙ„ØªØ±Ø´Ú©Ù†'\n",
    "ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "id": "dT5llfEvA724"
   },
   "outputs": [],
   "source": [
    "def detect_with_useless_persian_letters(token = \"Ø³Ù„Ø§Ø§Ø§Ø§Ø§Ù…\", illegal_words=['Ø³Ù„Ø§Ù…','Ø³Ù„Ø§Ù…']):\n",
    "\n",
    "  regex = r\"([^\\W\\d_])\\1{1,}\"\n",
    "  subst = \"\\\\1\"\n",
    "\n",
    "  for illegal_word in illegal_words:\n",
    "    # You can manually specify the number of replacements by changing the 4th argument\n",
    "    result = re.sub(regex, subst, token, 0, re.MULTILINE)\n",
    "    # print(result, illegal_word)\n",
    "\n",
    "    if result == illegal_word:\n",
    "      return illegal_word\n",
    "\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtkB36TeM_S-"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªØ§Ø¨Ø¹ number_convertor </h4>\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ù‡ Ú©Ù…Ú© Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÛŒ num2fawords Ø§Ø¹Ø¯Ø§Ø¯ Ø±Ø§ Ø¨Ù‡ Ø´Ú©Ù„ Ù†ÙˆØ´ØªØ§Ø±ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¨Ø¹Ø¯Ø§ Ø§Ù…Ú©Ø§Ù† ØªØ´Ø®ÛŒØµ Ø­Ø§Ù„Ø§ØªÛŒ Ù…Ø§Ù†Ù†Ø¯ Û³Û°Ú¯Ø§Ø± Ø¨Ø¬Ø§ÛŒ Ú©Ù„Ù…Ù‡â€ŒÛŒ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø³ÛŒÚ¯Ø§Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "id": "ttBVdRhvBDVy"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from num2fawords import words, ordinal_words\n",
    "\n",
    "import string\n",
    "def find_whitespace(st):\n",
    "    for index, character in enumerate(st):\n",
    "      if character in string.whitespace:\n",
    "            yield index\n",
    "\n",
    "\n",
    "def number_convertor(main_text = \"Ú†Ù†Ø¯ Ø¹Ø¯Ø¯ 30Ú¯Ø§Ø± Ú¯Ø±ÙØªÙ…\"):\n",
    "\n",
    "  main_text = main_text.replace('\\u202F', ' ') # replace half spaces with regular spaces\n",
    "\n",
    "  number_indexes = [] # indexes of all numbers in text\n",
    "  number_lengths = [] # length of numbers in text\n",
    "  new_whitespace_indices = []\n",
    "  output_list = []\n",
    "  \"\"\"\n",
    "  Takes a text which may contains a word as combination of number and subtext,\n",
    "  then converts the 'number' part to its corrosponding text.\n",
    "\n",
    "  the only input is a simple text.\n",
    "\n",
    "  https://pypi.org/project/num2fawords/\n",
    "  \"\"\"\n",
    "\n",
    "  output = ''\n",
    "\n",
    "  normalizer = Normalizer()\n",
    "  tokens = word_tokenize(normalizer.normalize(main_text))\n",
    "  # print(tokens)\n",
    "\n",
    "  whitespace_indices = list(find_whitespace(main_text))\n",
    "  # print(whitespace_indices)\n",
    "\n",
    "  temp = None\n",
    "\n",
    "  for item in tokens:\n",
    "    try:\n",
    "      words(item)\n",
    "      temp = words(item)\n",
    "      if item == '.':\n",
    "        temp = '.'\n",
    "      number_lengths.append(len(temp))\n",
    "      number_indexes.append(normalizer.normalize(main_text).index(item))\n",
    "      index = normalizer.normalize(main_text).index(item)\n",
    "      length = len(item)\n",
    "\n",
    "      length_temp = len(temp)\n",
    "\n",
    "      output_list.append([item, [index, index + length], [index, index + length_temp]])\n",
    "\n",
    "    \n",
    "    except: \n",
    "      if temp:\n",
    "        output += temp+item+' '\n",
    "        temp = None\n",
    "\n",
    "      else:\n",
    "        # pass\n",
    "        output += item+' '\n",
    "  \n",
    "  # for space_position in whitespace_indices:\n",
    "  #   if space_position > number_indexes[0]:\n",
    "  #     space_position = space_position + number_lengths[0]\n",
    "\n",
    "  # new_whitespace_indices.append(space_position)\n",
    "  # print(number_indexes)\n",
    "  # print(new_whitespace_indices)\n",
    "\n",
    "  if output[-1] == ' ':\n",
    "    output = output[:-1] + ''\n",
    "  return output, output_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSkWvchqAkI8"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> ØªØ§Ø¨Ø¹ emoji_convertor </h4>\n",
    "Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ø¨Ù‡ Ú©Ù…Ú© Ø¯ÛŒØªØ§Ø³ØªÛŒ Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù‡Ø§ Ú©Ù‡ unicode \n",
    "Ù…ØªÙ†Ø§Ø¸Ø± Ø¨Ø§ Ù‡Ø±ÛŒÚ© Ø§Ø² Ø¢Ù† Ù‡Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªØŒ Ø¨Ù‡ ØªØ¨Ø¯ÛŒÙ„ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¬Ù…Ù„Ù‡ Ø¨Ù‡ Ú©Ù„Ù…Ù‡ Ù…ØªÙ†Ø§Ø¸Ø± Ø¨Ø§ Ø¢Ù† Ù…ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø¯. \n",
    "Ø¨Ø¯ÛŒÙ† ØªØ±ØªÛŒØ¨ Ø§Ù…Ú©Ø§Ù† ØªØ´Ø®ÛŒØµ \"ğŸ˜ØªØ±Ø´Ú©Ù†\"\n",
    "Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ù‡ ØºÛŒØ± Ù‚Ø§Ù†ÙˆÙ†ÛŒ\n",
    "'ÙÛŒÙ„ØªØ±Ø´Ú©Ù†'\n",
    "Ø®ÙˆØ§Ù‡Ø¯ Ø¯Ø§Ø´Øª.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "id": "v7fRxLYBBLzL"
   },
   "outputs": [],
   "source": [
    "def emoji_convertor(main_text=\"ğŸ˜ØªØ±Ø´Ú©Ù†\"):\n",
    "\n",
    "  output = None\n",
    "  temp_text = main_text\n",
    "\n",
    "  output_list = []\n",
    "\n",
    "  \"\"\"\n",
    "  Takes a text which may contains a word as combination of emoji and subtext,\n",
    "  then converts the 'emoji' part to its corrosponding text.\n",
    "\n",
    "  https://github.com/skorani/Preprocess-Emoji\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # for colab\n",
    "  try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "\n",
    "    !git clone https://github.com/skorani/Preprocess-Emoji.git\n",
    "    !cp /content/Preprocess-Emoji/emojies_data.py /content/emojies_data.py\n",
    "    !python /content/Preprocess-Emoji/emojies_data.py\n",
    "\n",
    "  except:\n",
    "    IN_COLAB = False\n",
    "  \n",
    "  from emojies_data import EMOJI_ALIAS_UNICODE\n",
    "  \n",
    "  main_unicode_code_points = [f\"\\\\U{x:08x}\" for x in map(ord, main_text)]\n",
    "  main_unicode_text = \"\".join(main_unicode_code_points)\n",
    "\n",
    "  for item in main_unicode_code_points:\n",
    "    for k, v in EMOJI_ALIAS_UNICODE.items():\n",
    "\n",
    "      text = v\n",
    "      unicode_code_points = [f\"\\\\U{x:08x}\" for x in map(ord, text)]\n",
    "      unicode_text = \"\".join(unicode_code_points)\n",
    "\n",
    "      if unicode_text == item:\n",
    "        # print(len((str(k[1:-1]))))  #string of emoji  \n",
    "        # if item in main_unicode_code_points: #index of emoji\n",
    "        #   print(main_unicode_code_points.index(item))\n",
    "\n",
    "        # index = main_unicode_text.rfind(item)\n",
    "        index = main_unicode_code_points.index(item)\n",
    "        # Slice string to remove character at index \n",
    "        if len(main_text) > index:\n",
    "          main_text = main_text[0 : index : ] + main_text[index + 1 : :]\n",
    "\n",
    "        main_text = main_text[:index] + k[1:-1] + main_text[index:]\n",
    "        length = len((str(k[1:-1])))\n",
    "        emoji_word = str(k[1:-1])\n",
    "        output = main_text\n",
    "\n",
    "        output_list.append([emoji_word, [index, index+1], [index, index + length]])\n",
    "\n",
    "  if output is None:\n",
    "    output = temp_text\n",
    "\n",
    "  # output_list.insert(0, output)\n",
    "  return output, output_list\n",
    "  \n",
    "# print(emoji_convertor(main_text='ğŸ˜ØªØ±Ø´Ú©Ù† Ø³Ù„Ø§Ù…     *Ù*Ú¯ ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ   Ùˆ ØºÛŒØ±Ù‡ 30Ú¯Ø§Ø± Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMUFazgVY-Ub"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> Ú©Ù„Ø§Ø³ DetectIlligalWords </h4>\n",
    "Ú©Ù„Ø§Ø³ Ø§ØµÙ„ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ Ù†ÙˆØªâ€ŒØ¨ÙˆÚ© ØªÙˆØ¶ÛŒØ­ Ø¢Ù† Ø¯Ø§Ø¯Ù‡ Ø´Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "id": "jHkkwTdrPBjo"
   },
   "outputs": [],
   "source": [
    "class DetectIlligalWords:\n",
    "  def __init__(self, mode = 1):\n",
    "    # currently there is no different mode\n",
    "    self.mode = mode\n",
    "    self.original_text = \"\"\n",
    "    self.converted_text = \"\"\n",
    "    self.emoji_idxes = []\n",
    "    self.num_idxes = []\n",
    "    self.current_map = []\n",
    "\n",
    "  def preprocess(self, input, convertor = 'emoji'):\n",
    "    self.original_text = input\n",
    "    # normalize the input text\n",
    "    normalizer = Normalizer()\n",
    "    text = normalizer.normalize(input)\n",
    "    self.normalized_text = text\n",
    "\n",
    "    if convertor == 'emoji':\n",
    "      #convert emojis\n",
    "      text, self.emoji_idxes = emoji_convertor(text)\n",
    "      self.converted_text = text\n",
    "    elif convertor == 'number':\n",
    "      text, self.num_idxes = number_convertor(text)\n",
    "      self.converted_text = text\n",
    "\n",
    "    # tokenize the normalized text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens,text\n",
    "\n",
    "  def find_idx(self, token, converted_text, idx_map):\n",
    "    tmp_idx = converted_text.index(token)\n",
    "    idx = [tmp_idx, tmp_idx + len(token) - 1]\n",
    "\n",
    "    for i in idx_map:\n",
    "      real = i[1]\n",
    "      unreal = i[2]\n",
    "      if idx[1] >= real[0]:\n",
    "        idx[1] -= unreal[1] - real[1] \n",
    "\n",
    "      if idx[0] >= real[1]:\n",
    "        idx[0] -= unreal[1] - real[1] \n",
    "\n",
    "    return tuple(idx)\n",
    "\n",
    "  def merge_dics(self, dic1, dic2):\n",
    "    keys = set(dic1.keys()).union(set(dic2.keys()))\n",
    "    merged_dic = {}\n",
    "    for key in keys:\n",
    "      value = set()\n",
    "      if key in dic1:\n",
    "        value = value.union(dic1[key])\n",
    "      if key in dic2:\n",
    "        value = value.union(dic2[key])\n",
    "\n",
    "      merged_dic[key] = value\n",
    "\n",
    "    return merged_dic \n",
    "\n",
    "  def detect_funcs(self, token, illegal_words):\n",
    "    illegal_word = None\n",
    "\n",
    "    illegal_word = detect_with_useless_persian_letters(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is useless perisan')\n",
    "      return illegal_word\n",
    "\n",
    "    illegal_word = detect_non_persian_alternative(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is non persian alternative')\n",
    "      return illegal_word\n",
    "\n",
    "    illegal_word = detect_with_star(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is star')\n",
    "      return illegal_word\n",
    "\n",
    "    illegal_word = detect_fingilish(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is fingilish')\n",
    "      return illegal_word\n",
    "\n",
    "\n",
    "    illegal_word = detect_typo(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is typo')\n",
    "      return illegal_word\n",
    "\n",
    "\n",
    "  def detectIlligalWordsInTokens(self, all_tokens, illegal_words, text):\n",
    "    detected_illegal_words = {}\n",
    "    illegal_word = None\n",
    "    tokens = [] # contains meaningless tokens\n",
    "    illegal_tokens = {}\n",
    "\n",
    "\n",
    "    for token in all_tokens:\n",
    "      if token in illegal_words:\n",
    "        # print('this is simple case')\n",
    "        illegal_word = token\n",
    "\n",
    "        if illegal_word:\n",
    "          illegal_tokens[illegal_word] = set([token])\n",
    "          illegal_word = None\n",
    "\n",
    "      elif not is_meaningful_word(token): # removing meaningful words\n",
    "        tokens.append(token)\n",
    "\n",
    "    for token in tokens:\n",
    "      illegal_word = self.detect_funcs(token, illegal_words)\n",
    "\n",
    "      if illegal_word:\n",
    "        if illegal_word not in illegal_tokens:\n",
    "          illegal_tokens[illegal_word] = set([token])\n",
    " \n",
    "        else:\n",
    "          illegal_tokens[illegal_word].add(token)\n",
    "        illegal_word = None\n",
    "\n",
    "    for illegal_word, tokens in illegal_tokens.items():\n",
    "      for token in tokens:\n",
    "        idx = self.find_idx(token, self.converted_text, self.current_map)\n",
    "\n",
    "        if illegal_word not in detected_illegal_words:\n",
    "          detected_illegal_words[illegal_word] = set()\n",
    "      \n",
    "        detected_illegal_words[illegal_word].add(idx)\n",
    "\n",
    "    return detected_illegal_words\n",
    "\n",
    "\n",
    "  def detectIlligalWordsInText(self, text, illegal_words):\n",
    "    detected_illegal_words = {}\n",
    "\n",
    "    detected_illegal_words = detect_with_spaces(self, text, illegal_words)\n",
    "\n",
    "    return detected_illegal_words\n",
    "\n",
    "\n",
    "  def detect(self, tokens, illegal_words, text):\n",
    "    detected_words_tokens = self.detectIlligalWordsInTokens(tokens, illegal_words, text)\n",
    "\n",
    "    detected_words_text = self.detectIlligalWordsInText(text, illegal_words)\n",
    "\n",
    "    detected_words = self.merge_dics(detected_words_tokens, detected_words_text) \n",
    "\n",
    "    return detected_words\n",
    "\n",
    "  def process(self, tokens, illegal_words):\n",
    "    detected_original_words = self.detect(tokens, illegal_words, self.converted_text)\n",
    "\n",
    "    # lemmatization and detection\n",
    "    lemmatizer = Lemmatizer()\n",
    "    all_tokens_lemma = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    detected_lemma_words = self.detect(tokens, illegal_words, self.converted_text)\n",
    "\n",
    "    detected_words = self.merge_dics(detected_original_words, detected_lemma_words)\n",
    "\n",
    "    return detected_words\n",
    "\n",
    "  def run(self, input: str, illegal_words: list):\n",
    "    # print('###------ with emoji --------###')\n",
    "    tokens, text = self.preprocess(input, convertor = 'emoji')\n",
    "    self.current_map = self.emoji_idxes\n",
    "    detected_words_with_emoji = self.process(tokens, illegal_words)\n",
    "    \n",
    "    # print('###------ with number --------###')\n",
    "    tokens, text = self.preprocess(input, convertor = 'number')\n",
    "    self.current_map = self.num_idxes\n",
    "    detected_words_with_number = self.process(tokens, illegal_words)\n",
    "\n",
    "    detected_words = self.merge_dics(detected_words_with_emoji, detected_words_with_number)\n",
    "\n",
    "    print(\"****************FINAL RESULT****************\")\n",
    "    print(detected_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqqJallkZkc8"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h3> ØªØ³Øª Ù†Ù‡Ø§ÛŒÛŒ </h3>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù‚Ø³Ù…Øª Ø®Ø±ÙˆØ¬ÛŒ ØªØ³Øª Ù†Ù‡Ø§ÛŒÛŒ Ú©Ø§Ø± Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„ÙÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯. Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ù…Ø«Ø§Ù„ Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ† Ú©Ù„Ù…Ù‡â€ŒÛŒ Ú†Ø§Ù‚Ùˆ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú©Ù„Ù…Ù‡â€ŒÛŒ ØºÛŒØ±Ù…Ø¬Ø§Ø²ØŒ Ø§Ú¯Ø± Ø¯Ø± Ù…ØªÙ† Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ… \n",
    "\"Ø¬Ø§ğŸ¦¢\" \n",
    "Ú©Ù‡ ÛŒÚ© Ø§Ø´ØªØ¨Ø§Ù‡ ØªØ§ÛŒÙ¾ÛŒ Ùˆ ÛŒÚ© Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¯Ø± Ø¢Ù† ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø¢Ù† Ø±Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ù‡ÛŒÙ….\n",
    "<br>\n",
    "Ù‡Ù…Ú†Ù†ÛŒÙ† Ø¯Ø± Ø­Ø§Ù„Øª Ø³Ø®Øªâ€ŒØªØ± Ùˆ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±ÛŒÙ…:\n",
    "<br>\n",
    "\"Ø§ÛŒÙ† Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ ØªØ³Øª ÛŒØ§ÙØªÙ† Ú©Ù„Ù…Ø§Øª ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø§Ù†Ù†Ø¯ ØªÙÙ†Ú¯ Ùˆ ğŸ˜ØªØ±Ø´Ú©Ù† Ùˆ ØºÛŒØ±Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯. Ø§Ø² Ø¯ÛŒÚ¯Ø± Ù…ÙˆØ§Ø±Ø¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ Øª**Ú¯ Ùˆ Û³Û°Ú¯Ø§Ø± Ùˆ Ø¨mØ¨ Ùˆ Ù…Ø«Ù„Ø§ ØªÙ‚Ù†Ú¯ Ùˆ ØªÙÛµÙ†*Ú¯ Ùˆ ØªÙ Ù†Ú¯ Ùˆ Ø¨ Ù… Ø¨ Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯\"\n",
    "<br>\n",
    "Ø¯Ø± Ø§ÛŒÙ† Ù…ØªÙ† Ø§Ù†ÙˆØ§Ø¹ Ùˆ Ø§Ù‚Ø³Ø§Ù… Ø¨Ù‡ Ú©Ø§Ø±Ú¯ÛŒØ±ÛŒ Ú©Ù„Ù…Ø§Øª ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª Ú©Ù‡ ØªÙ…Ø§Ù…ÛŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ index Ø¢Ù†â€ŒÙ‡Ø§ ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12MVr9tBPnTk",
    "outputId": "e3ed3c71-add4-46cc-d028-905a81db34da"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DetectIlligalWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20508/2278687650.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0millegalWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'ØªÙÙ†Ú¯'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ÙÛŒÙ„ØªØ±Ø´Ú©Ù†'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Ú†Ø§Ù‚Ùˆ'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Ø³ÛŒÚ¯Ø§Ø±'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Ø¨Ù…Ø¨'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdetectIlligalWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDetectIlligalWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mdetectIlligalWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0millegalWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DetectIlligalWords' is not defined"
     ]
    }
   ],
   "source": [
    "text = 'Ø§ÛŒÙ† Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ ØªØ³Øª ÛŒØ§ÙØªÙ† Ú©Ù„Ù…Ø§Øª ØºÛŒØ±Ù‚Ø§Ù†ÙˆÙ†ÛŒ Ù…Ø§Ù†Ù†Ø¯ ØªÙÙ†Ú¯ Ùˆ ğŸ˜ØªØ±Ø´Ú©Ù† Ùˆ ØºÛŒØ±Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø®ÙˆØ§Ù‡Ø¯ Ø´Ø¯. Ø§Ø² Ø¯ÛŒÚ¯Ø± Ù…ÙˆØ§Ø±Ø¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ù‡ Øª**Ú¯ Ùˆ Û³Û°Ú¯Ø§Ø± Ùˆ Ø¨mØ¨ Ùˆ Ù…Ø«Ù„Ø§ ØªÙ‚Ù†Ú¯ Ùˆ ØªÙÛµÙ†*Ú¯ Ùˆ ØªÙ Ù†Ú¯ Ùˆ Ø¨ Ù… Ø¨ Ø§Ø´Ø§Ø±Ù‡ Ú©Ø±Ø¯'\n",
    "# text = 'Ø³Ù„Ø§Ù… ğŸ˜ØªØ±Ø´Ú©Ù†'\n",
    "# text = 'Ø³Ù„Ø§Ù… Û³Û° Ú¯Ø§#Ø±'\n",
    "# text = 'Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒ Ø¨ Ù… Ø¨ Ø±Ùˆ ØªØ´Ø®ÛŒØµ Ø¨Ø¯ÛŒØŸ'\n",
    "# text = 'sigar Ø¨mØ¨ '\n",
    "# text = 'Ø¬Ø§ğŸ¦¢'\n",
    "# text = 'ØªÙÚ¯'\n",
    "# text = 'Ú†Ø§Ø§Ø§Ø§Ù‚Ùˆ'\n",
    "# text = 'Ú†.Ø§.Ù‚.Ùˆ'\n",
    "# text = 'filtershekan'\n",
    "\n",
    "illegalWords = ['ØªÙÙ†Ú¯','ÙÛŒÙ„ØªØ±Ø´Ú©Ù†','Ú†Ø§Ù‚Ùˆ','Ø³ÛŒÚ¯Ø§Ø±','Ø¨Ù…Ø¨']\n",
    "\n",
    "detectIlligalWords = DetectIlligalWords()\n",
    "detectIlligalWords.run(text,illegalWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
