{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrSIAHfR7g_H"
   },
   "source": [
    "<div dir=\"rtl\" align=\"center\">\n",
    "<font face=\"XB Zar\" size=5>\n",
    "    <font face=\"IranNastaliq\" size=5>\n",
    "      به نام خدا\n",
    "    </font>\n",
    "    <br>\n",
    "    <font size=3>\n",
    "      دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
    "    </font>\n",
    "    <br>\n",
    "    <font color=blue size=5>\n",
    "      پردازش زبان‌های طبیعی\n",
    "    </font>\n",
    "    <br>\n",
    "    <hr/>\n",
    "    <font color=red size=6>\n",
    "      تمرین دوم: تشخیص کلمات غیرقانونی\n",
    "    </font>\n",
    "    <br>\n",
    "      حدیث احمدیان، سینا عبدوس، مهسا یزدانی\n",
    "    <hr>\n",
    "<br>\n",
    "  <div align=\"right\">\n",
    "\n",
    "  <div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgQQ7gWv-XYV"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h1>تعریف مسئله</h1>\n",
    "<div>ما در این تمرین می‌خواهیم به عنوان ورودی یک متن و یک لیست از کلمات غیر قانونی بگیریم و تشخیص دهیم که آیا در این متن، این کلمات غیرقانونی وجود دارند یا خیر و اگر وجود داشتند، بازه‌ی هر کلمه‌ی غیرقانونی را نیز مشخص کنیم.</div>\n",
    "\n",
    "<h2>کلاس اصلی</h2>\n",
    "<div> یک کلاس اصلی به نام DetectIlligalWords وجود دارد که تابعی به نام run، تابع اصلی آن است. این تابع ابتدا تابع preprocess و سپس تابع process را فراخوانی می‌کند.\n",
    "\n",
    "<h4> تابع preprocess </h4>\n",
    "با فراخوانی تابع run ابتدا تابع preprocess فراخوانی می‌شوند.\n",
    "در تابع preprocess ابتدا normalization انجام شده و همچنین ایموجی‌های موجود در متن به متن تعریف می‌شوند. سپس روی این متن tokenization انجام می‌شود و خروجی تابع preprocess لیستی از tokenها خواهد بود.\n",
    "\n",
    "نکته: تبدیل ایموجی به متن به این دلیل است که ممکن است حالتی مانند 🐘ترشکن ایجاد شود که در آن کلمه غیرقانونی فیلترشکن همراه با یک ایموجی نوشته شده است. برای تبدیل ایموجی به متن، از دیتاستی استفاده شده است که هر ایموجی را به عبارتی که آن را توصیف می‌کند، map شده است.\n",
    "\n",
    "<h4> تابع process </h4>\n",
    "در این تابع ابتدا تابع detect روی خود tokenها فراخوانی شده و سپس یک بار هم روی lemmaی tokenها فراخوانی می‌شود. چرا که ممکن است به طور مثال بجای خود کلمه‌ی تفنگ در متن داشته باشیم تفنگ‌ها.\n",
    "تابع detect تعدادی دیگری از توابع را فراخوانی می‌کند که در نهایت هر کدام از آن‌ها حالت‌های مختلفی از به کار بردن کلمه‌ی غیرقانونی را تشخیص می‌دهند که در ادامه هر کدام از این توابع و مثال‌هایی از مواردی که تشخیص می‌دهند، توضیح داده خواهد شد.\n",
    "\n",
    "</div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJpCFRbLC_1E"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<hr/>\n",
    "در سلول‌های بعدی، ماژول‌های مورد استفاده که در فایل requirements.txt نوشته شده‌اند، ابتدا نصب و سپس import می‌شوند.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTegHxYx4wrM"
   },
   "outputs": [],
   "source": [
    "from typing_extensions import Text\n",
    "!touch /content/requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "udG-vaK3BVlQ",
    "outputId": "de360d90-24e6-4d88-82a7-c84cc4c70591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /content/requirments.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /content/requirments.txt\n",
    "\n",
    "hazm\n",
    "num2fawords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh4KOe7x5Ctn",
    "outputId": "bf2d531a-3571-4b5c-e34b-41ae088c6de2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: hazm in /usr/local/lib/python3.9/dist-packages (from -r /content/requirments.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: num2fawords in /usr/local/lib/python3.9/dist-packages (from -r /content/requirments.txt (line 3)) (1.1)\n",
      "Requirement already satisfied: libwapiti>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from hazm->-r /content/requirments.txt (line 2)) (0.2.1)\n",
      "Requirement already satisfied: nltk==3.3 in /usr/local/lib/python3.9/dist-packages (from hazm->-r /content/requirments.txt (line 2)) (3.3)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from nltk==3.3->hazm->-r /content/requirments.txt (line 2)) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r /content/requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EXebGbB8cRgz"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s5gNTwL6D0EC"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h3> توابع استفاده شده </h3>\n",
    "<h4> تابع is_meaningful_word </h4>\n",
    "به منظور اینکه اشتباهی در تشخیص کلمات غیرقانونی پیش نیاید و کلمه‌ای معنی‌دار به عنوان کلمه‌ای غیرقانونی که در بین حروف آن کاراکتر‌های فارسی خاص به کار رفته است تشخیص داده نشود، پس از تشخیص حالت ابتدایی کلمات غیرقانونی (یعنی به شکل مستقیم به کار رفتن آن‌ها)، tokenهای معنی دار از لیست tokenها حذف خواهند شد.\n",
    "<br>\n",
    "به این منظور فایلی به نام persian_words.txt در پوشه‌ی پروژه قرار داده شده است که لیستی از کلمات معنادار فارسی است و برای چک کردن معنی‌دار بودن یا نبودن یک کلمه، چک می‌شود که آیا کلمه در این لیست هست یا خیر.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aJBk0xiIaNQG",
    "outputId": "68fad3c1-afff-4612-a49a-a375d3f2ca91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1FGMH4pzNmN3x8WE4vGpd03F95O4pITcL\n",
      "To: /content/persian_words.txt\n",
      "100% 25.8M/25.8M [00:00<00:00, 142MB/s] \n"
     ]
    }
   ],
   "source": [
    "# read persian words from text file and convert to a list\n",
    "%cd /content\n",
    "!gdown 1FGMH4pzNmN3x8WE4vGpd03F95O4pITcL\n",
    "\n",
    "with open('/content/persian_words.txt', 'r', encoding=\"utf8\") as f:\n",
    "    persian_words = [line.strip() for line in f.read().splitlines()]\n",
    "\n",
    "# cheks whether the token is meaningful or not\n",
    "def is_meaningful_word(token):\n",
    "  if token in persian_words:\n",
    "      return True\n",
    "  return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HUda75tE-nt"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> تابع detect_with_star </h4>\n",
    "این تابع تابعی است که حالاتی را چک می‌کند که در آن یک یا تعدادی از حروف کلمه‌ی غیرقانونی با * جایگزین شده‌اند. به عنوان مثال بجای تفنگ حالتی مانند ت**گ نوشته شده است.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2G6e58pE1rk"
   },
   "outputs": [],
   "source": [
    "# examples: \"تف*گ ت**گ *ف*گ نف*گ\" for \"تفنگ\"\n",
    "def detect_with_star(token, illegal_words):\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  To check asterisk occurrence in the word.\n",
    "  Illegal and input words should have the same length\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  if token == '.':\n",
    "    return None\n",
    "\n",
    "  for illegal_word in illegal_words:\n",
    "\n",
    "    if len(token) < len(illegal_word):\n",
    "      return None\n",
    "\n",
    "    pattern = re.compile(token.replace(\"*\", \"[[\\u0600-\\u06FF]|گ|چ|پ|ژ]\"), re.IGNORECASE)\n",
    "    match = re.search(pattern, illegal_word)\n",
    "    if match:\n",
    "      return illegal_word\n",
    "    else:\n",
    "      return None\n",
    "\n",
    "# detect_with_star('نگ', ['تفنگ'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YSlPM2EXh5V6"
   },
   "outputs": [],
   "source": [
    "def all_formats_a(word,all,indx): \n",
    "  all.add(word)\n",
    "  flag=True\n",
    "  for i in range(indx,len(word)):\n",
    "    if word[i]==\"a\" or word[i]==\"A\":\n",
    "      flag=False\n",
    "      all_formats_a(word,all,i+1)\n",
    "      all_formats_a(word[:i]+word[i+1:],all,i)\n",
    "  return all\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq8hVfp9VK7c"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> توابع  all_formats_o , all_formats_a </h4>\n",
    "این توابع پیش پردازش های لازم برای شناسایی کلمات فینگلیش را انجام میدهند. در فارسی ما اعراب کلمات را نمینویسیم اما در انگلیسی با کاراکترهایی مانند o و a و e اعراب ها را نیز مینویسیم، حال آنکه بعضی از این کاراکتر ها (مثل a) میتواند هم برای اعراب به کار رود و هم به عنوان حرف الف استفاده شود. پس باید یک سری پیش پردازش روی کلمه ی فینگلیش انجام داد تا به درستی این کاراکترها با کاراکتر های کلمه ی غیرقانونی مقایسه شوند و حالات مختلف آنها (اینکه اعراب بودند یا معادل یک مصوت آمده اند) در نظر گرفته شود\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2U7YcNmLb1K"
   },
   "outputs": [],
   "source": [
    "def all_formats_o(word,all,indx):\n",
    "  all.add(word)\n",
    "  for i in range(indx,len(word)):\n",
    "    if word[i]==\"o\" or word[i]==\"O\":\n",
    "      all_formats_o(word[:i]+\"v\"+word[i+1:],all,i+1)\n",
    "      all_formats_o(word[:i]+word[i+1:],all,i)\n",
    "\n",
    "  return all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIsyVNcyWC2F"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> توابع  ENG_to_format , PRS_to_format </h4>\n",
    "این توابع نیز پیش پردازش های لازم برای شناسایی کلمات فینگلیش را انجام میدهند. همانطور که گفته شد کلمات فارسی و فینگلیش در نگارش خود تفاوت هایی دارند که مقایسه ی آنها به راحتی و با مقایسه ی حرف به حرف ممکن نیست و باید تفاوت های آن ها را درنظر گرفت. حالات زیادی وجود دارد که در فینگلیش نوشته میشود ولی در فارسی نه (مثل اعراب) و حالات زیادی وجود دارد که در فارسی نوشته میشود ولی در انگلیسی نه! مانند ع اول کلمه (مثلا عامل =َamel)\n",
    "به همین علت باید کلمه ی غیرقانونی که به فارسی نوشته شده و کلمه ی فینگلیش هردو به یک فرمتی برده شوند که قابلیت مقایسه باهم را داشته باشند. توابع زیر این کار را انجام میدهند.\n",
    "مثال های دیگری از یکسان سازی فرمت که توابع زیر انجام میدهد  مانند مصوت های زیادی که معادل یک صدا هستند مثل ض ز ظ ذ ولی در انگلیسی همگی z نوشته میشود.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMcfis-Ye5SB"
   },
   "outputs": [],
   "source": [
    "def ENG_to_format(word):\n",
    "\n",
    "  if word[0]==\"e\" or word[0]==\"o\":\n",
    "    word=\"@\"+word[1:]\n",
    "  if word[0]==\"a\" or word[0]==\"i\" or word[0]==\"o\":\n",
    "    word=\"@\"+word\n",
    "  if word[-1]==\"e\":\n",
    "    word=word[:len(word)-1]+\"h\"\n",
    "\n",
    "  word=word.replace(\"oo\",\"v\")\n",
    "  \n",
    "  word=word.replace(\"e\",\"\")\n",
    "  word=word.replace(\"u\",\"v\")\n",
    "  word=word.replace(\"i\",\"y\")\n",
    "  word=word.replace(\"ee\",\"y\")\n",
    "\n",
    "  all=set([word])\n",
    "  all=all_formats_a(word,all,0)\n",
    "  all_list=list(all)\n",
    "\n",
    "  for x in all_list:\n",
    "    all_formats_o(x,all,0)\n",
    "\n",
    "  all_list=list(all)\n",
    "\n",
    "\n",
    "  res=[]\n",
    "\n",
    "  for x in all_list:\n",
    "    res.append(x.replace(\"o\",\"\"))\n",
    "  return(set(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nm3cCt8RmtNa"
   },
   "outputs": [],
   "source": [
    "def PRS_to_format(word):\n",
    "\n",
    "  res=\"\"\n",
    "  if word[0]==\"ع\" or word[0]==\"آ\" or word[0]==\"ا\":\n",
    "    res+=\"@\"\n",
    "\n",
    "  for w in word:\n",
    "    if w==\"ا\":\n",
    "      res+=\"a\"\n",
    "    elif w==\"ب\":\n",
    "      res+=\"b\"\n",
    "    elif w==\"پ\":\n",
    "      res+=\"p\"\n",
    "    elif w==\"ت\" or w==\"ط\":\n",
    "      res+=\"t\"\n",
    "    elif w==\"ث\" or w==\"ص\" or w==\"س\":\n",
    "      res+=\"s\"\n",
    "    elif w==\"ج\":\n",
    "      res+=\"j\"\n",
    "    elif w==\"چ\":\n",
    "      res+=\"ch\"\n",
    "    elif w==\"ح\":\n",
    "      res+=\"h\"\n",
    "    elif w==\"خ\":\n",
    "      res+=\"kh\"\n",
    "    elif w==\"د\":\n",
    "      res+=\"d\"\n",
    "    elif w==\"ذ\"or w==\"ض\"or w==\"ز\"or w==\"ظ\":\n",
    "      res+=\"z\"\n",
    "    elif w==\"ر\":\n",
    "      res+=\"r\"\n",
    "    elif w==\"ژ\":\n",
    "      res+=\"zh\"\n",
    "    elif w==\"ش\":\n",
    "      res+=\"sh\"\n",
    "    elif w==\"ق\"or w==\"غ\":\n",
    "      res+=\"q\"\n",
    "    elif w==\"ف\":\n",
    "      res+=\"f\"\n",
    "    elif w==\"ک\":\n",
    "      res+=\"k\"\n",
    "    elif w==\"گ\":\n",
    "      res+=\"g\"\n",
    "    elif w==\"ل\":\n",
    "      res+=\"l\"\n",
    "    elif w==\"م\":\n",
    "      res+=\"m\"\n",
    "    elif w==\"ن\":\n",
    "      res+=\"n\"\n",
    "    elif w==\"و\":\n",
    "      res+=\"v\"\n",
    "    elif w==\"ه\" or w==\"ح\":\n",
    "      res+=\"h\"\n",
    "    elif w==\"ی\":\n",
    "      res+=\"y\"\n",
    "  return res\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQabcBDoesCF"
   },
   "outputs": [],
   "source": [
    "def check_FiEnglish(word,illegal):\n",
    "  if PRS_to_format(illegal) in list(word):\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRaPkY0MGU_R"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> تابع detect_fingilish </h4>\n",
    "این تابع مواردی که در آن کلمه‌ی غیرقانونی به شکل فینگیلش نوشته شده باشد را تشخیص می‌دهد. به عنوان مثال اگر برای کلمه‌ی غیرقانونی فیلترشکن داشته باشیم: filtershekan\n",
    "در این تابع ابتدا کلمه ی غیرقانونی و توکن هردو به فرمت مورد نظر برده میشوند و با check_finglish مقایسه میشود که آیا توکن فینگلیش معادل غیرقانونی بوده است یا نه.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KRgVGFg3iaxN"
   },
   "outputs": [],
   "source": [
    "def detect_fingilish(token, illegal_words):\n",
    "  word=ENG_to_format(token)\n",
    "  for ill in illegal_words:\n",
    "    if check_FiEnglish(word,ill):\n",
    "      return ill\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_o1XR0QYF5jy"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> تابع detect_typo </h4>\n",
    "این تابع برای تشخیص غلط تایپی با این قانون است که یک حرف از حروف کلمه ی غیرقانونی با یک کاراکتر اشتباه جایگزین شده باشد یا یک حرف پاک شده باشد. مثل فیلترشکن و فیلترشکم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kxc9fGANOUqJ"
   },
   "outputs": [],
   "source": [
    "def detect_typo(token, illegal_words):\n",
    "  for ill in illegal_words:\n",
    "    if len(token)==len(ill) and sum(c1 != c2 for c1, c2 in zip(token, ill))==1:\n",
    "      return ill\n",
    "    elif len(token)==len(ill)-1:\n",
    "      if sum(c1 != c2 for c1, c2 in zip(\"@\"+token, ill))==1 or sum(c1 != c2 for c1, c2 in zip(token+\"@\", ill))==1:\n",
    "          return ill\n",
    "      for i in range(len(token)-1):\n",
    "        if sum(c1 != c2 for c1, c2 in zip(token[:i+1]+\"@\"+token[i+1:], ill))==1:\n",
    "          return ill\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hYlVLQ9QYbtk"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> تابع make_regex </h4>\n",
    "این تابع کلمه ی غیر قانونی را دریافت کرده و یک regular expression برای آن مینویسد. این RE قرار است در ادامه حالت های مختلفی از نوشتن کلمه ی غیر قانونی را شامل شود که به شرح زیر هستند\n",
    "</br>\n",
    "1- حالات مصوت های هم صدا (درواقع غلط املایی) : صیگار یا ثیگار به جای سیگار\n",
    "</br>\n",
    "2- حالات حروف هم صدای انگلیسی به جای صدای فارسی مثل sیگار به جای سیگار</br>\n",
    "3- حالات حروف غیر فارسی بین حروف کلمه مانند س%یگا$$ر</br>\n",
    "4- هر ترکیبی از حالات فوق، مثلا صی&g#ار</br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qq6hSsKGesQC"
   },
   "outputs": [],
   "source": [
    "def make_regex(word,gp):\n",
    "  a=\"[a,A,ا,آ]\"\n",
    "  b=\"[b,B,ب]\"\n",
    "  p=\"[p,P,پ]\"\n",
    "  t=\"[t,T,ط,ت]\"\n",
    "  s=\"[s,S,c,S,س,ص,ث]\"\n",
    "  j=\"[j,J,g,G,ج]\"\n",
    "  ch=\"[ch|CH|cH|Ch|چ]\"\n",
    "  h=\"[h,H,ح,ه]\"\n",
    "  kh=\"[kh|kH|KH|Kh|خ]\"\n",
    "  d=\"[d|D|د]\"\n",
    "  z=\"[z|Z|ز|ض|ذ|ظ]\"\n",
    "  r=\"[r|R|ر]\"\n",
    "  zh=\"[zH|ZH|Zh|zh|ژ|j]\"\n",
    "  q=\"[q|Q|ق|غ]\"\n",
    "  g=\"[g|G|گ]\"\n",
    "  m=\"[m|M|م]\"\n",
    "  v=\"[v|W|w|V|u|U|و|o|O|oo|Oo|oO]\"\n",
    "  y=\"[i|I|Y|y|ی]\"\n",
    "  f=\"[f,F,ف]\"\n",
    "  l=\"[l,L,ل]\"\n",
    "  t=\"[t,T,ت,ط]\"\n",
    "  sh=\"[sh|ش|Sh|SH|sH]\"\n",
    "  k=\"[k,K,c,C,ک]\"\n",
    "  n=\"[n,N,ن]\"\n",
    "\n",
    "\n",
    "  res=\"\"\n",
    "  for w in word:\n",
    "    if w==\"آ\" or w==\"ا\":\n",
    "      res+=a+gp\n",
    "    elif w==\"ب\":\n",
    "      res+=b+gp\n",
    "    elif w==\"پ\":\n",
    "      res+=p+gp\n",
    "    elif w==\"ت\" or w==\"ط\":\n",
    "      res+=t+gp\n",
    "    elif w==\"ث\" or w==\"ص\" or w==\"س\":\n",
    "      res+=s+gp\n",
    "    elif w==\"ج\":\n",
    "      res+=j+gp\n",
    "    elif w==\"چ\":\n",
    "      res+=ch+gp\n",
    "    elif w==\"ح\":\n",
    "      res+=h+gp\n",
    "    elif w==\"خ\":\n",
    "      res+=kh+gp\n",
    "    elif w==\"د\":\n",
    "      res+=d+gp\n",
    "    elif w==\"ذ\"or w==\"ض\"or w==\"ز\"or w==\"ظ\":\n",
    "      res+=z+gp\n",
    "    elif w==\"ر\":\n",
    "      res+=r+gp\n",
    "    elif w==\"ژ\":\n",
    "      res+=zh+gp\n",
    "    elif w==\"ش\":\n",
    "      res+=sh+gp\n",
    "    elif w==\"ق\"or w==\"غ\":\n",
    "      res+=q+gp\n",
    "    elif w==\"ف\":\n",
    "      res+=f+gp\n",
    "    elif w==\"ک\":\n",
    "      res+=k+gp\n",
    "    elif w==\"گ\":\n",
    "      res+=g+gp\n",
    "    elif w==\"ل\":\n",
    "      res+=l+gp\n",
    "    elif w==\"م\":\n",
    "      res+=m+gp\n",
    "    elif w==\"ن\":\n",
    "      res+=n+gp\n",
    "    elif w==\"و\":\n",
    "      res+=v+gp\n",
    "    elif w==\"ه\" or w==\"ح\":\n",
    "      res+=h+gp\n",
    "    elif w==\"ی\":\n",
    "      res+=y+gp\n",
    "\n",
    "  return res[:len(res)-len(gp)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n7eKbX8FE_X9"
   },
   "outputs": [],
   "source": [
    "def if_illegal_alter(txt,illegal):\n",
    "  x = re.search(illegal, txt)\n",
    "  if x:\n",
    "    return True\n",
    "  else:\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7mne-rEGvyi"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> تابع detect_non_persian_alternative </h4>\n",
    "این تابع توکن را با RE کلمه ی غیرقانونی که در بالا توضیح داده شد مقایسه میکند که اگر یکی از حالات توضیح داده شده رخ داده باشد توکن را به عنوان کلمه غیرقانونی معرفی میکند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubzhR5_qnE6w"
   },
   "outputs": [],
   "source": [
    "def detect_non_persian_alternative(token, illegal_words):\n",
    "  non_pers=\"[^پ,چ,ج,ح,خ,ه,ع,غ,ف,ق,ث,ص,ض,گ,ک,م,ن,ت,ا,ل,ب,ی,س,ش,و,ئ,د,ذ,ر,ز,ط,ظ,آ,ة,ي,ژ,ؤ,إ,أ,ء,ۀ]*\"\n",
    "  for ill in illegal_words:\n",
    "    reg=make_regex(ill,non_pers)\n",
    "    if if_illegal_alter(token,reg):\n",
    "      return ill\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xO1XcKBaOfg"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> تابع detect_with_spaces </h4>\n",
    "این تابع حالاتی را هندل میکند که بین کلمه ی غیرقانونی علاوه بر کاراکتر غیرفارسی فاصله باشد مثلا فی لتر شکن\n",
    "</br>\n",
    "نکته1: علت جدا کردن این تابع از RE قبلی این هست که در توکن کردن بر حسب فاصله بخش های مخلف کلمه ی جدا شده با اسپیس از هم جدا میشوند و باید یک تابع جدا برایش تعریف کرد\n",
    "</br>\n",
    "نکته2: این تابع حالات خاص جایگزینی مصوت ها را هم هندل میکند مثل صی gار\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {
    "id": "qMf8J1rMOol5"
   },
   "outputs": [],
   "source": [
    "def detect_with_spaces(self,text,illegal_words):\n",
    "  res=dict([])\n",
    "  for ill in illegal_words:\n",
    "    reg=make_regex(ill,\"[^پ,چ,ج,ح,خ,ه,ع,غ,ف,ق,ث,ص,ض,گ,ک,م,ن,ت,ا,ل,ب,ی,س,ش,و,ئ,د,ذ,ر,ز,ط,ظ,آ,ة,ي,ژ,ؤ,إ,أ,ء,ۀ]*\")\n",
    "    for m in re.finditer(reg, text):\n",
    "      mIdx=self.find_idx(text[m.start():m.end()],text,self.current_map)\n",
    "      if not ill in res:\n",
    "        res[ill]=set([mIdx])\n",
    "      else:\n",
    "        res[ill].add(mIdx)\n",
    "      \n",
    "  return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6l7t06aHAbh5"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> تابع detect_with_useless_persian_letters </h4>\n",
    "این تابع حالاتی را هندل میکند که هر یک از حروف کلمه غیر قانونی، در جای خود تکرار داشته باشند.\n",
    "بدین ترتیب امکان تشخیص \"فیلللللترشکککککن\"\n",
    "برای کلمه غیر قانونی\n",
    "'فیلترشکن'\n",
    "وجود دارد.\n",
    "</br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "id": "dT5llfEvA724"
   },
   "outputs": [],
   "source": [
    "def detect_with_useless_persian_letters(token = \"سلااااام\", illegal_words=['سلام','سلام']):\n",
    "\n",
    "  regex = r\"([^\\W\\d_])\\1{1,}\"\n",
    "  subst = \"\\\\1\"\n",
    "\n",
    "  for illegal_word in illegal_words:\n",
    "    # You can manually specify the number of replacements by changing the 4th argument\n",
    "    result = re.sub(regex, subst, token, 0, re.MULTILINE)\n",
    "    # print(result, illegal_word)\n",
    "\n",
    "    if result == illegal_word:\n",
    "      return illegal_word\n",
    "\n",
    "  return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FtkB36TeM_S-"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> تابع number_convertor </h4>\n",
    "این تابع به کمک کتابخانه‌ی num2fawords اعداد را به شکل نوشتاری آن‌ها تبدیل می‌کند تا بعدا امکان تشخیص حالاتی مانند ۳۰گار بجای کلمه‌ی غیرقانونی سیگار وجود داشته باشد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "id": "ttBVdRhvBDVy"
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "from hazm import *\n",
    "from num2fawords import words, ordinal_words\n",
    "\n",
    "import string\n",
    "def find_whitespace(st):\n",
    "    for index, character in enumerate(st):\n",
    "      if character in string.whitespace:\n",
    "            yield index\n",
    "\n",
    "\n",
    "def number_convertor(main_text = \"چند عدد 30گار گرفتم\"):\n",
    "\n",
    "  main_text = main_text.replace('\\u202F', ' ') # replace half spaces with regular spaces\n",
    "\n",
    "  number_indexes = [] # indexes of all numbers in text\n",
    "  number_lengths = [] # length of numbers in text\n",
    "  new_whitespace_indices = []\n",
    "  output_list = []\n",
    "  \"\"\"\n",
    "  Takes a text which may contains a word as combination of number and subtext,\n",
    "  then converts the 'number' part to its corrosponding text.\n",
    "\n",
    "  the only input is a simple text.\n",
    "\n",
    "  https://pypi.org/project/num2fawords/\n",
    "  \"\"\"\n",
    "\n",
    "  output = ''\n",
    "\n",
    "  normalizer = Normalizer()\n",
    "  tokens = word_tokenize(normalizer.normalize(main_text))\n",
    "  # print(tokens)\n",
    "\n",
    "  whitespace_indices = list(find_whitespace(main_text))\n",
    "  # print(whitespace_indices)\n",
    "\n",
    "  temp = None\n",
    "\n",
    "  for item in tokens:\n",
    "    try:\n",
    "      words(item)\n",
    "      temp = words(item)\n",
    "      if item == '.':\n",
    "        temp = '.'\n",
    "      number_lengths.append(len(temp))\n",
    "      number_indexes.append(normalizer.normalize(main_text).index(item))\n",
    "      index = normalizer.normalize(main_text).index(item)\n",
    "      length = len(item)\n",
    "\n",
    "      length_temp = len(temp)\n",
    "\n",
    "      output_list.append([item, [index, index + length], [index, index + length_temp]])\n",
    "\n",
    "    \n",
    "    except: \n",
    "      if temp:\n",
    "        output += temp+item+' '\n",
    "        temp = None\n",
    "\n",
    "      else:\n",
    "        # pass\n",
    "        output += item+' '\n",
    "  \n",
    "  # for space_position in whitespace_indices:\n",
    "  #   if space_position > number_indexes[0]:\n",
    "  #     space_position = space_position + number_lengths[0]\n",
    "\n",
    "  # new_whitespace_indices.append(space_position)\n",
    "  # print(number_indexes)\n",
    "  # print(new_whitespace_indices)\n",
    "\n",
    "  if output[-1] == ' ':\n",
    "    output = output[:-1] + ''\n",
    "  return output, output_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hSkWvchqAkI8"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> تابع emoji_convertor </h4>\n",
    "این تابع به کمک دیتاستی از ایموجی ها که unicode \n",
    "متناظر با هریک از آن ها موجود است، به تبدیل ایموجی موجود در جمله به کلمه متناظر با آن می پردازد. \n",
    "بدین ترتیب امکان تشخیص \"🐘ترشکن\"\n",
    "برای کلمه غیر قانونی\n",
    "'فیلترشکن'\n",
    "خواهد داشت.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {
    "id": "v7fRxLYBBLzL"
   },
   "outputs": [],
   "source": [
    "def emoji_convertor(main_text=\"🐘ترشکن\"):\n",
    "\n",
    "  output = None\n",
    "  temp_text = main_text\n",
    "\n",
    "  output_list = []\n",
    "\n",
    "  \"\"\"\n",
    "  Takes a text which may contains a word as combination of emoji and subtext,\n",
    "  then converts the 'emoji' part to its corrosponding text.\n",
    "\n",
    "  https://github.com/skorani/Preprocess-Emoji\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  # for colab\n",
    "  try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "\n",
    "    !git clone https://github.com/skorani/Preprocess-Emoji.git\n",
    "    !cp /content/Preprocess-Emoji/emojies_data.py /content/emojies_data.py\n",
    "    !python /content/Preprocess-Emoji/emojies_data.py\n",
    "\n",
    "  except:\n",
    "    IN_COLAB = False\n",
    "  \n",
    "  from emojies_data import EMOJI_ALIAS_UNICODE\n",
    "  \n",
    "  main_unicode_code_points = [f\"\\\\U{x:08x}\" for x in map(ord, main_text)]\n",
    "  main_unicode_text = \"\".join(main_unicode_code_points)\n",
    "\n",
    "  for item in main_unicode_code_points:\n",
    "    for k, v in EMOJI_ALIAS_UNICODE.items():\n",
    "\n",
    "      text = v\n",
    "      unicode_code_points = [f\"\\\\U{x:08x}\" for x in map(ord, text)]\n",
    "      unicode_text = \"\".join(unicode_code_points)\n",
    "\n",
    "      if unicode_text == item:\n",
    "        # print(len((str(k[1:-1]))))  #string of emoji  \n",
    "        # if item in main_unicode_code_points: #index of emoji\n",
    "        #   print(main_unicode_code_points.index(item))\n",
    "\n",
    "        # index = main_unicode_text.rfind(item)\n",
    "        index = main_unicode_code_points.index(item)\n",
    "        # Slice string to remove character at index \n",
    "        if len(main_text) > index:\n",
    "          main_text = main_text[0 : index : ] + main_text[index + 1 : :]\n",
    "\n",
    "        main_text = main_text[:index] + k[1:-1] + main_text[index:]\n",
    "        length = len((str(k[1:-1])))\n",
    "        emoji_word = str(k[1:-1])\n",
    "        output = main_text\n",
    "\n",
    "        output_list.append([emoji_word, [index, index+1], [index, index + length]])\n",
    "\n",
    "  if output is None:\n",
    "    output = temp_text\n",
    "\n",
    "  # output_list.insert(0, output)\n",
    "  return output, output_list\n",
    "  \n",
    "# print(emoji_convertor(main_text='🐘ترشکن سلام     *ف*گ غیرقانونی   و غیره 30گار خواهد شد.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMUFazgVY-Ub"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h4> کلاس DetectIlligalWords </h4>\n",
    "کلاس اصلی است که در ابتدای نوت‌بوک توضیح آن داده شد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {
    "id": "jHkkwTdrPBjo"
   },
   "outputs": [],
   "source": [
    "class DetectIlligalWords:\n",
    "  def __init__(self, mode = 1):\n",
    "    # currently there is no different mode\n",
    "    self.mode = mode\n",
    "    self.original_text = \"\"\n",
    "    self.converted_text = \"\"\n",
    "    self.emoji_idxes = []\n",
    "    self.num_idxes = []\n",
    "    self.current_map = []\n",
    "\n",
    "  def preprocess(self, input, convertor = 'emoji'):\n",
    "    self.original_text = input\n",
    "    # normalize the input text\n",
    "    normalizer = Normalizer()\n",
    "    text = normalizer.normalize(input)\n",
    "    self.normalized_text = text\n",
    "\n",
    "    if convertor == 'emoji':\n",
    "      #convert emojis\n",
    "      text, self.emoji_idxes = emoji_convertor(text)\n",
    "      self.converted_text = text\n",
    "    elif convertor == 'number':\n",
    "      text, self.num_idxes = number_convertor(text)\n",
    "      self.converted_text = text\n",
    "\n",
    "    # tokenize the normalized text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    return tokens,text\n",
    "\n",
    "  def find_idx(self, token, converted_text, idx_map):\n",
    "    tmp_idx = converted_text.index(token)\n",
    "    idx = [tmp_idx, tmp_idx + len(token) - 1]\n",
    "\n",
    "    for i in idx_map:\n",
    "      real = i[1]\n",
    "      unreal = i[2]\n",
    "      if idx[1] >= real[0]:\n",
    "        idx[1] -= unreal[1] - real[1] \n",
    "\n",
    "      if idx[0] >= real[1]:\n",
    "        idx[0] -= unreal[1] - real[1] \n",
    "\n",
    "    return tuple(idx)\n",
    "\n",
    "  def merge_dics(self, dic1, dic2):\n",
    "    keys = set(dic1.keys()).union(set(dic2.keys()))\n",
    "    merged_dic = {}\n",
    "    for key in keys:\n",
    "      value = set()\n",
    "      if key in dic1:\n",
    "        value = value.union(dic1[key])\n",
    "      if key in dic2:\n",
    "        value = value.union(dic2[key])\n",
    "\n",
    "      merged_dic[key] = value\n",
    "\n",
    "    return merged_dic \n",
    "\n",
    "  def detect_funcs(self, token, illegal_words):\n",
    "    illegal_word = None\n",
    "\n",
    "    illegal_word = detect_with_useless_persian_letters(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is useless perisan')\n",
    "      return illegal_word\n",
    "\n",
    "    illegal_word = detect_non_persian_alternative(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is non persian alternative')\n",
    "      return illegal_word\n",
    "\n",
    "    illegal_word = detect_with_star(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is star')\n",
    "      return illegal_word\n",
    "\n",
    "    illegal_word = detect_fingilish(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is fingilish')\n",
    "      return illegal_word\n",
    "\n",
    "\n",
    "    illegal_word = detect_typo(token, illegal_words)\n",
    "    if illegal_word:\n",
    "      # print('this is typo')\n",
    "      return illegal_word\n",
    "\n",
    "\n",
    "  def detectIlligalWordsInTokens(self, all_tokens, illegal_words, text):\n",
    "    detected_illegal_words = {}\n",
    "    illegal_word = None\n",
    "    tokens = [] # contains meaningless tokens\n",
    "    illegal_tokens = {}\n",
    "\n",
    "\n",
    "    for token in all_tokens:\n",
    "      if token in illegal_words:\n",
    "        # print('this is simple case')\n",
    "        illegal_word = token\n",
    "\n",
    "        if illegal_word:\n",
    "          illegal_tokens[illegal_word] = set([token])\n",
    "          illegal_word = None\n",
    "\n",
    "      elif not is_meaningful_word(token): # removing meaningful words\n",
    "        tokens.append(token)\n",
    "\n",
    "    for token in tokens:\n",
    "      illegal_word = self.detect_funcs(token, illegal_words)\n",
    "\n",
    "      if illegal_word:\n",
    "        if illegal_word not in illegal_tokens:\n",
    "          illegal_tokens[illegal_word] = set([token])\n",
    " \n",
    "        else:\n",
    "          illegal_tokens[illegal_word].add(token)\n",
    "        illegal_word = None\n",
    "\n",
    "    for illegal_word, tokens in illegal_tokens.items():\n",
    "      for token in tokens:\n",
    "        idx = self.find_idx(token, self.converted_text, self.current_map)\n",
    "\n",
    "        if illegal_word not in detected_illegal_words:\n",
    "          detected_illegal_words[illegal_word] = set()\n",
    "      \n",
    "        detected_illegal_words[illegal_word].add(idx)\n",
    "\n",
    "    return detected_illegal_words\n",
    "\n",
    "\n",
    "  def detectIlligalWordsInText(self, text, illegal_words):\n",
    "    detected_illegal_words = {}\n",
    "\n",
    "    detected_illegal_words = detect_with_spaces(self, text, illegal_words)\n",
    "\n",
    "    return detected_illegal_words\n",
    "\n",
    "\n",
    "  def detect(self, tokens, illegal_words, text):\n",
    "    detected_words_tokens = self.detectIlligalWordsInTokens(tokens, illegal_words, text)\n",
    "\n",
    "    detected_words_text = self.detectIlligalWordsInText(text, illegal_words)\n",
    "\n",
    "    detected_words = self.merge_dics(detected_words_tokens, detected_words_text) \n",
    "\n",
    "    return detected_words\n",
    "\n",
    "  def process(self, tokens, illegal_words):\n",
    "    detected_original_words = self.detect(tokens, illegal_words, self.converted_text)\n",
    "\n",
    "    # lemmatization and detection\n",
    "    lemmatizer = Lemmatizer()\n",
    "    all_tokens_lemma = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    detected_lemma_words = self.detect(tokens, illegal_words, self.converted_text)\n",
    "\n",
    "    detected_words = self.merge_dics(detected_original_words, detected_lemma_words)\n",
    "\n",
    "    return detected_words\n",
    "\n",
    "  def run(self, input: str, illegal_words: list):\n",
    "    # print('###------ with emoji --------###')\n",
    "    tokens, text = self.preprocess(input, convertor = 'emoji')\n",
    "    self.current_map = self.emoji_idxes\n",
    "    detected_words_with_emoji = self.process(tokens, illegal_words)\n",
    "    \n",
    "    # print('###------ with number --------###')\n",
    "    tokens, text = self.preprocess(input, convertor = 'number')\n",
    "    self.current_map = self.num_idxes\n",
    "    detected_words_with_number = self.process(tokens, illegal_words)\n",
    "\n",
    "    detected_words = self.merge_dics(detected_words_with_emoji, detected_words_with_number)\n",
    "\n",
    "    print(\"****************FINAL RESULT****************\")\n",
    "    print(detected_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqqJallkZkc8"
   },
   "source": [
    "<div dir='rtl' style='direction:rtl:'>\n",
    "<h3> تست نهایی </h3>\n",
    "در این قسمت خروجی تست نهایی کار برای حالت‌های مختلفی وجود دارد. به عنوان مثال با در نظر گرفتن کلمه‌ی چاقو به عنوان کلمه‌ی غیرمجاز، اگر در متن داشته باشیم \n",
    "\"جا🦢\" \n",
    "که یک اشتباه تایپی و یک استفاده از ایموجی در آن وجود دارد، می‌توانیم آن را تشخیص دهیم.\n",
    "<br>\n",
    "همچنین در حالت سخت‌تر و به عنوان یک متن کامل می‌توانیم متن زیر را در نظر بگیریم:\n",
    "<br>\n",
    "\"این متن برای تست یافتن کلمات غیرقانونی مانند تفنگ و 🐘ترشکن و غیره استفاده خواهد شد. از دیگر موارد می‌توان به ت**گ و ۳۰گار و بmب و مثلا تقنگ و تف۵ن*گ و تف نگ و ب م ب اشاره کرد\"\n",
    "<br>\n",
    "در این متن انواع و اقسام به کارگیری کلمات غیرقانونی آمده است که تمامی آن‌ها به همراه index آن‌ها تشخیص داده شده‌اند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12MVr9tBPnTk",
    "outputId": "e3ed3c71-add4-46cc-d028-905a81db34da"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DetectIlligalWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20508/2278687650.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0millegalWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'تفنگ'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'فیلترشکن'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'چاقو'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'سیگار'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'بمب'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[0mdetectIlligalWords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDetectIlligalWords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mdetectIlligalWords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0millegalWords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DetectIlligalWords' is not defined"
     ]
    }
   ],
   "source": [
    "text = 'این متن برای تست یافتن کلمات غیرقانونی مانند تفنگ و 🐘ترشکن و غیره استفاده خواهد شد. از دیگر موارد می‌توان به ت**گ و ۳۰گار و بmب و مثلا تقنگ و تف۵ن*گ و تف نگ و ب م ب اشاره کرد'\n",
    "# text = 'سلام 🐘ترشکن'\n",
    "# text = 'سلام ۳۰ گا#ر'\n",
    "# text = 'می‌تونی ب م ب رو تشخیص بدی؟'\n",
    "# text = 'sigar بmب '\n",
    "# text = 'جا🦢'\n",
    "# text = 'تفگ'\n",
    "# text = 'چااااقو'\n",
    "# text = 'چ.ا.ق.و'\n",
    "# text = 'filtershekan'\n",
    "\n",
    "illegalWords = ['تفنگ','فیلترشکن','چاقو','سیگار','بمب']\n",
    "\n",
    "detectIlligalWords = DetectIlligalWords()\n",
    "detectIlligalWords.run(text,illegalWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
