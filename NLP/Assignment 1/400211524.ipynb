{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=5><div dir=rtl align=center>\n",
        "<font face=\"IranNastaliq\" size=5>\n",
        "به نام خدا\n",
        "</font>\n",
        "<br>\n",
        "<font size=3>\n",
        "دانشگاه صنعتی شریف - دانشکده مهندسی کامپیوتر\n",
        "</font>\n",
        "<br>\n",
        "<font color=green size=5>\n",
        "پردازش زبان های طبیعی - تمرین اول\n",
        "</font>\n",
        "\n",
        "<hr/>\n",
        "<font color=blue size=5>\n",
        "‌ حدیث احمدیان\n",
        "<br>\n",
        "400211524\n",
        "<hr>\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "6YMrIfoBOdpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"red\" size=5>1. دانلود متون و توضیحاتی درمورد آن ها</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "JP3noUjG-xVR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "*** به دلیل سرعت بد اینترنت نتوانستم از هیچکدام از پلتفرم هایی که برای آپلود فایل لینک دانلود مستقیم در اختیار قرار میدهند استفاده کنم. فایل متون در کنار تکلیف ضمیمه شده است لطفا برای اجرا آن ها را در دایرکتوری اجرای نوت بوک قرار دهید. ***\n",
        "\n",
        "اگر به هردلیلی در دانلود مشکلی پیش آمد، فایل متن ها در پوشه ای کنار این نوت بوک ضمیمه شده است.\n",
        "<br>\n",
        "متن های انتخابی در واقع فیلم نامه ی پنج تا از فیلم مورد علاقه ی بنده هست و هدف یافتن عناصر مشترک بین آن هاست.\n",
        "<br>\n",
        "فیلمنامه ها از وبسایت https://www.scriptslug.com/script/mr-nobody-2009 دانلود شده اند که ابتدا به صورت pdf بوده اند و با استفاده از لینک https://pdftotext.com/ به فرمت txt تبدیل شده اند.\n",
        "<br>\n",
        "نکته ی مورد توجه در مورد فایل فیلمنامه این هست که صفحه ی اول آن مختص به عنوان، نام نویسنده و ... میباشد پس قبل از استفاده از فایل ها برای این نوت بوک، صفحه ی اول آن ها حذف شده است.\n",
        "<br>\n",
        "فیلم های انتخابی به این قرار هستند:\n",
        "<br>\n",
        "- Eternal Sunshine of the Spotless Mind\n",
        "- Her\n",
        "- Interstellar\n",
        "- La La land\n",
        "- Mr. nobody\n",
        "</font>"
      ],
      "metadata": {
        "id": "oT4VkXzP-ovi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"red\" size=5>2. Import کردن کتابخانه های موردنیاز و نصب پکیج های مورد نیاز</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "PzKndlniBkyC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "در ذیل کتابخانه ها و پکیج های مورد نیاز نصب و import شده اند."
      ],
      "metadata": {
        "id": "x-bdj38qBzNw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jJw92lh4tQu_"
      },
      "outputs": [],
      "source": [
        "import random \n",
        "import tqdm\n",
        "import nltk, re\n",
        "from nltk.corpus import brown\n",
        "from nltk import word_tokenize\n",
        "import string\n",
        "from nltk.probability import FreqDist\n",
        "import numpy as np\n",
        "import itertools\n",
        "from nltk import FreqDist\n",
        "import itertools\n",
        "import pandas as pd\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download(\"omw-1.4\")\n",
        "nltk.download(\"brown\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ksEnn04S9S7",
        "outputId": "f9bed48f-a740-4ca5-92cc-e3211d95dd56"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"red\" size=5>3. تعریف توابع مورد نیاز</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "QNAcoynMCAPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "در ذیل توابع موردنیاز تعریف شده اند که هرکدام توضیح داده خواهد شد.\n",
        "<br>\n",
        "نکته ی قابل توجه این است که اکثریت این کدها از نوت بوک نمونه ی کلاس موجود در لینک گیتهاب برداشته شده و بخش های اندکی از آن بنابر مورد استفاده تغییر داده شده که توضیح داده خواهد شد."
      ],
      "metadata": {
        "id": "8houGAMwCEiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"blue\" size=5>تابع tokenBySent</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "tKwJKanXCe-L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "این تابع متن را دریافت میکند و سپس با sent_tokenize که بالاتر import شده است، جملات متن را جدا میکند و در متغیر sent میریزد سپس با استفاده از پترن های RE تعریف شده توکن های هر جمله را جداسازی میکند. دروافع خروجی این تابع لیستی از لیست ها هست که هر لیست یک جمله و محتویات آن توکن های آن جمله است."
      ],
      "metadata": {
        "id": "Za6T39m-CrIa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenBySent(txt):\n",
        "  sents=sent_tokenize(txt)\n",
        "  # http://stackoverflow.com/questions/36353125/nltk-regular-expression-tokenizer\n",
        "  pattern = r'''(?x)          # set flag to allow verbose regexps\n",
        "          (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
        "        | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
        "        | \\$?\\d+(?:\\.\\d+)?%?\\s?  # currency and percentages, e.g. $12.40, 82%\n",
        "        | \\.\\.\\.              # ellipsis\n",
        "        | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
        "      '''\n",
        "\n",
        "  pattern = re.compile(pattern)\n",
        "\n",
        "  sents_tocken=[]\n",
        "  for s in sents:\n",
        "    sents_tocken.append(nltk.regexp_tokenize(s, pattern))\n",
        "  return sents_tocken"
      ],
      "metadata": {
        "id": "n5Kp32rK1K64"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"blue\" size=5>تابع normalize_sentence</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "2HpvxTTvEE0K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "این تابع خروجی تابع قبلی را میگیرد و نرمالایز میکند به این صورت که همه حروف را lowercase میکند، stopword ها و punctuation ها را حذف میکند.\n",
        "تفاوت های این تابع با تابع نوت بوک های کلاس تنها در بخش حذف stopword ها هست. با به این صورت که اگر کاربر علاوه برا stopwordها پیش فرض stopwordی وارد کند، بدون lowercase شدن با متن مقایسه و حذف خواهد شد. علت این امر این است که عباراتی در فیلم نامه ها وجود دارند که همگی uppercase هستند و اگر lowercase شوند ممکن است با کلمات عادی دیگری اشتباه گرفته شوند. مانند EXIT که برای پایان یک صحنه است و exit که میتواند برای صرفا در جمله ای محاوره ای به کار رفته باشد.\n",
        "<br>\n",
        "خروجی این بخش دقیقا به فرمت ورودی است با این تفاوات که نرمالایز شده."
      ],
      "metadata": {
        "id": "v8g3gKvtEFJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_sentence(tokenized_sents, minimum_length=2, stopword_removal=True, stopwords_domain=[], lower_case=True, punctuation_removal=True):\n",
        "    '''\n",
        "    normalization function\n",
        "    '''\n",
        "    normalized_sents = tokenized_sents\n",
        "    \n",
        "    if stopword_removal:\n",
        "        # Remove stopwords in English and also the given domain stopwords\n",
        "        stopwords = [x.lower() for x in nltk.corpus.stopwords.words('english')]\n",
        "        normalized_sents=[[word for word in sentence if (word.lower() not in stopwords)] for sentence in tokenized_sents ]\n",
        "        normalized_sents=[[word for word in sentence if (word not in stopwords_domain)] for sentence in normalized_sents ]\n",
        "\n",
        "    if punctuation_removal:\n",
        "        # Remove punctuations\n",
        "        normalized_sents=[[word for word in sentence if word not in string.punctuation] for sentence in normalized_sents ]\n",
        "\n",
        "    if lower_case:\n",
        "        # Convert everything to lowercase and filter based on a min length\n",
        "        normalized_sents=[[word.lower() for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]\n",
        "\n",
        "    elif minimum_length>1:\n",
        "        normalized_sents= [[word for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]        \n",
        "        \n",
        "    return normalized_sents"
      ],
      "metadata": {
        "id": "B7V01uWHSJxW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"blue\" size=5>تابع properties</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "ZtDPGThUFjXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "این تابع لیستی از کلمات میگیرد و تعداد کلمات، کلمات یکتا، میانیگن طول کلمات و جملات و طولانی ترین کلمه ی متن را چاپ میکند."
      ],
      "metadata": {
        "id": "Ws1SWEOFFmLa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def properties(normed_w):\n",
        "  sentence_words = list(itertools.chain(*normed_w))\n",
        "  print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
        "  print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
        "  avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
        "  print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
        "  avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
        "  print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
        "  print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])\n",
        "  print(\"---------------------------------\\n\")"
      ],
      "metadata": {
        "id": "FsIfABFyHwRk"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"blue\" size=5>تابع get_lemma_set</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "BcN-7sX7HVLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "این تابع توکن ها را دریافت میکند و با توجه به آپشن ورودی opt یا آن ها را با PorterStemer، stem میکند و یا با WorldNetLemmatizer، Lemmatize میکند."
      ],
      "metadata": {
        "id": "fOjrh67nHVi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "stemmer  = PorterStemmer()\n",
        "\n",
        "\n",
        "def get_lemma_set(tok, opt=2):\n",
        "    if opt ==1:\n",
        "        return stemmer.stem(tok)\n",
        "    if opt ==2:\n",
        "        return lemmatizer.lemmatize(tok)"
      ],
      "metadata": {
        "id": "BMrSyabkJpia"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"red\" size=5>4. پردازش متن</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "GmNtvdRhKY15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "در این قسمت محتوای فایل ها خوانده شده و در متغیرهای نظیر خود ریخته میشوند."
      ],
      "metadata": {
        "id": "uk9JrNhKIPlH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('eternal-sunshine-of-the-spotless-mind-2004.txt', 'r') as file:\n",
        "    eternal = file.read()\n",
        "with open('her-2013.txt', 'r') as file:\n",
        "    her = file.read()\n",
        "with open('interstellar-2014.txt', 'r') as file:\n",
        "    interstellar = file.read()\n",
        "with open('la-la-land-2016.txt', 'r') as file:\n",
        "    lalaland = file.read()\n",
        "with open('mr-nobody-2009.txt', 'r') as file:\n",
        "    mrnobody = file.read()"
      ],
      "metadata": {
        "id": "OLg9wZ7RKXZj"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "در این قسمت هر متن با استفاده از توابع تعریف شده، توکنایز و نرمالایز میشود و سپس ویژگی های کلی آن (همان زول جملات و تعداد کلمات و...) چاپ میشود.\n",
        "<br>\n",
        "علاوه بر stopword های پیش فرض ما استاپ ..رد های زیر را نیز در نظر گرفتیم که همانگونه که توضیح دادیم موارد مربوط به توضیحات فیلمنامه و جابجایی بین سکانس ها و ... هستند و طبیعتا همه ی فیلمنامه ها این هارا دارند و ما میخواهیم متن اصلی را تحلیل کنیم پس این ها را حذف میکنیم.\n",
        "<br>\n",
        "این stopwordها عبارت هستند از :\n",
        "<br>\n",
        "\"INT\",\"...\",\"CONT’D\",\"CONT\",\"EXT\",\"EXIT\""
      ],
      "metadata": {
        "id": "lftHC1qUIjbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sents_tocken_eternal=tokenBySent(eternal)\n",
        "normed_w_eternal=normalize_sentence(sents_tocken_eternal,stopwords_domain=[\"INT\",\"...\",\"CONT’D\",\"CONT\",\"EXT\",\"EXIT\"])\n",
        "properties(normed_w_eternal)\n",
        "\n",
        "\n",
        "sents_tocken_her=tokenBySent(her)\n",
        "normed_w_her=normalize_sentence(sents_tocken_her,stopwords_domain=[\"INT\",\"...\",\"CONT’D\",\"CONT\",\"EXT\",\"EXIT\"])\n",
        "properties(normed_w_her)\n",
        "\n",
        "\n",
        "sents_tocken_interstellar=tokenBySent(interstellar)\n",
        "normed_w_interstellar=normalize_sentence(sents_tocken_interstellar,stopwords_domain=[\"INT\",\"...\",\"CONT’D\",\"CONT\",\"EXT\",\"EXIT\"])\n",
        "properties(normed_w_interstellar)\n",
        "\n",
        "\n",
        "sents_tocken_lalaland=tokenBySent(lalaland)\n",
        "normed_w_lalaland=normalize_sentence(sents_tocken_lalaland,stopwords_domain=[\"INT\",\"...\",\"CONT’D\",\"CONT\",\"EXT\",\"EXIT\"])\n",
        "properties(normed_w_lalaland)\n",
        "\n",
        "\n",
        "sents_tocken_mrnobody=tokenBySent(mrnobody)\n",
        "normed_w_mrnobody=normalize_sentence(sents_tocken_mrnobody,stopwords_domain=[\"INT\",\"...\",\"CONT’D\",\"CONT\",\"EXT\",\"EXIT\"])\n",
        "properties(normed_w_mrnobody)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OWeJgdNStZt",
        "outputId": "19816bab-424b-4cc8-b110-099ebf13f66d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words  13533           \n",
            "Number of unique words 2849            \n",
            "Average word length 5.665410478090593\n",
            "Average sentence length in characters 10.330820956181187\n",
            "Longest word     mock-sophisticated\n",
            "---------------------------------\n",
            "\n",
            "Number of words  9207            \n",
            "Number of unique words 2111            \n",
            "Average word length 5.915933528836755\n",
            "Average sentence length in characters 10.83186705767351\n",
            "Longest word     beautifulhandwrittenletters\n",
            "---------------------------------\n",
            "\n",
            "Number of words  14220           \n",
            "Number of unique words 3083            \n",
            "Average word length 5.787412095639944\n",
            "Average sentence length in characters 10.574824191279887\n",
            "Longest word     three-hundred-foot\n",
            "---------------------------------\n",
            "\n",
            "Number of words  9556            \n",
            "Number of unique words 2721            \n",
            "Average word length 5.580786940142319\n",
            "Average sentence length in characters 10.161573880284639\n",
            "Longest word     ahhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhht\n",
            "---------------------------------\n",
            "\n",
            "Number of words  16586           \n",
            "Number of unique words 3770            \n",
            "Average word length 5.485469673218377\n",
            "Average sentence length in characters 9.970939346436754\n",
            "Longest word     h-i-b-e-r-n-a-t-i-o-n\n",
            "---------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "در این قسمت خروجی های قسمت قبل را با استفاده از توابع تعریف شده، lemmatize میکنیم.\n",
        "<br>\n",
        "ابتدا باید لیست دو بعدی جملات- توکن ها را به یک لیست یک بعدی که شامل تمام توکن ها است تبدیل کنیم و سپس از تابع get_lemma_set استفاده کنیم.\n",
        "<br>\n",
        "علت استفاده lemmatizing این هست که بعدا میخواهیم ببینیم فیلم ها چه مفاهیم مشترکی داشتند پس بهتر است مفهوم کلمات حفظ شود."
      ],
      "metadata": {
        "id": "7NV2aa15JdIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt = 2\n",
        "eternal_all_token=list(itertools.chain(*normed_w_eternal))\n",
        "eternal_tokens_nonstop_lemstem = [get_lemma_set(t, opt) for t in tqdm.tqdm(eternal_all_token)]\n",
        "\n",
        "opt = 2\n",
        "her_all_token=list(itertools.chain(*normed_w_her))\n",
        "her_tokens_nonstop_lemstem =   [get_lemma_set(t, opt) for t in tqdm.tqdm(her_all_token)]\n",
        "\n",
        "opt = 2\n",
        "interstellar_all_token=list(itertools.chain(*normed_w_interstellar))\n",
        "interstellar_tokens_nonstop_lemstem =   [get_lemma_set(t, opt) for t in tqdm.tqdm(interstellar_all_token)]\n",
        "\n",
        "opt = 2\n",
        "lalaland_all_token=list(itertools.chain(*normed_w_lalaland))\n",
        "lalaland_tokens_nonstop_lemstem =   [get_lemma_set(t, opt) for t in tqdm.tqdm(lalaland_all_token)]\n",
        "\n",
        "opt = 2\n",
        "mrnobody_all_token=list(itertools.chain(*normed_w_mrnobody))\n",
        "mrnobody_tokens_nonstop_lemstem =   [get_lemma_set(t, opt) for t in tqdm.tqdm(mrnobody_all_token)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQfrsMIK-ueH",
        "outputId": "6a189800-abe6-4a74-ee28-a67c2fab89fc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13533/13533 [00:02<00:00, 4824.11it/s]\n",
            "100%|██████████| 9207/9207 [00:00<00:00, 97813.97it/s]\n",
            "100%|██████████| 14220/14220 [00:00<00:00, 205417.61it/s]\n",
            "100%|██████████| 9556/9556 [00:00<00:00, 198248.88it/s]\n",
            "100%|██████████| 16586/16586 [00:00<00:00, 232685.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"red\" size=5>5. تحلیل فرکانس</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "kpnkrHcYKI4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "در این بخش ما خروجی های قسمت قبل که درواقع توکنایز، نرمالایز و lemmatize شده ی متون ورودی هستند را بررسی میکنیم و 75 توکن از هرکدام که بیشترین تکرار را داشتند در قالب یک دیتافریم چاپ میکنیم.\n",
        "<br>\n",
        "طبیعتا نام شخصیت های فیلم جزو کلمات پرتکرار خواهد بود، پس برای اینکه به یک سری مفهوم پرتکرار به جز اسامی برسیم به جای 50 تا 75 تا پرتکرارترین را درنظر گرفتیم."
      ],
      "metadata": {
        "id": "_d1e7BccLESZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "dataframe = {}\n",
        "\n",
        "for opt in ['eternal','her','interstellar','lalaland','mrnobody']:\n",
        "     dataframe[opt] = FreqDist(eval(F\"{opt}_tokens_nonstop_lemstem\")).most_common(75)\n",
        "\n",
        "freq_analysis = pd.DataFrame(dataframe)  "
      ],
      "metadata": {
        "id": "Li9cv_xx4i3c"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_analysis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "CEBp8o1T5F17",
        "outputId": "637283b6-03cd-4d17-c9e3-54563bd7e844"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              eternal               her       interstellar          lalaland  \\\n",
              "0         (joel, 909)   (theodore, 698)      (cooper, 927)        (mia, 577)   \n",
              "1   (clementine, 590)   (samantha, 368)       (murph, 397)  (sebastian, 494)   \n",
              "2    (continued, 213)  (continued, 186)       (brand, 377)       (look, 137)   \n",
              "3         (mary, 176)       (know, 106)  (continuous, 225)    (revision, 84)   \n",
              "4         (stan, 167)         (amy, 90)        (look, 193)         (see, 81)   \n",
              "..                ...               ...                ...               ...   \n",
              "70       (memory, 29)       (could, 21)         (line, 27)       (dance, 19)   \n",
              "71         (take, 28)       (thing, 21)        (reach, 27)         (mom, 19)   \n",
              "72          (guy, 28)      (people, 21)        (space, 26)        (time, 19)   \n",
              "73        (train, 27)      (around, 21)         (away, 26)         (two, 19)   \n",
              "74        (smile, 27)    (actually, 21)          (ten, 26)      (people, 19)   \n",
              "\n",
              "        mrnobody  \n",
              "0   (nemo, 1131)  \n",
              "1    (anna, 247)  \n",
              "2     (day, 234)  \n",
              "3   (elise, 188)  \n",
              "4    (look, 173)  \n",
              "..           ...  \n",
              "70   (light, 33)  \n",
              "71    (want, 33)  \n",
              "72    (wake, 32)  \n",
              "73   (image, 32)  \n",
              "74   (flash, 32)  \n",
              "\n",
              "[75 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97b28160-2457-4abd-bf97-a88ce65070c9\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eternal</th>\n",
              "      <th>her</th>\n",
              "      <th>interstellar</th>\n",
              "      <th>lalaland</th>\n",
              "      <th>mrnobody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(joel, 909)</td>\n",
              "      <td>(theodore, 698)</td>\n",
              "      <td>(cooper, 927)</td>\n",
              "      <td>(mia, 577)</td>\n",
              "      <td>(nemo, 1131)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(clementine, 590)</td>\n",
              "      <td>(samantha, 368)</td>\n",
              "      <td>(murph, 397)</td>\n",
              "      <td>(sebastian, 494)</td>\n",
              "      <td>(anna, 247)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(continued, 213)</td>\n",
              "      <td>(continued, 186)</td>\n",
              "      <td>(brand, 377)</td>\n",
              "      <td>(look, 137)</td>\n",
              "      <td>(day, 234)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(mary, 176)</td>\n",
              "      <td>(know, 106)</td>\n",
              "      <td>(continuous, 225)</td>\n",
              "      <td>(revision, 84)</td>\n",
              "      <td>(elise, 188)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(stan, 167)</td>\n",
              "      <td>(amy, 90)</td>\n",
              "      <td>(look, 193)</td>\n",
              "      <td>(see, 81)</td>\n",
              "      <td>(look, 173)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>(memory, 29)</td>\n",
              "      <td>(could, 21)</td>\n",
              "      <td>(line, 27)</td>\n",
              "      <td>(dance, 19)</td>\n",
              "      <td>(light, 33)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>(take, 28)</td>\n",
              "      <td>(thing, 21)</td>\n",
              "      <td>(reach, 27)</td>\n",
              "      <td>(mom, 19)</td>\n",
              "      <td>(want, 33)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>(guy, 28)</td>\n",
              "      <td>(people, 21)</td>\n",
              "      <td>(space, 26)</td>\n",
              "      <td>(time, 19)</td>\n",
              "      <td>(wake, 32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>(train, 27)</td>\n",
              "      <td>(around, 21)</td>\n",
              "      <td>(away, 26)</td>\n",
              "      <td>(two, 19)</td>\n",
              "      <td>(image, 32)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>(smile, 27)</td>\n",
              "      <td>(actually, 21)</td>\n",
              "      <td>(ten, 26)</td>\n",
              "      <td>(people, 19)</td>\n",
              "      <td>(flash, 32)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>75 rows × 5 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97b28160-2457-4abd-bf97-a88ce65070c9')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97b28160-2457-4abd-bf97-a88ce65070c9 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97b28160-2457-4abd-bf97-a88ce65070c9');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"red\" size=5>6. یافتن مفاهیم مشترک</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "vUNpaYG9LiBn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "برای اینکه ببینیم فیلمنامه ها چه مفاهیم مشترکی داشتند، هرستون از دیتافریم فوق را به صورت یک مجموعه در نظر میگیریم و سپس هر توکنی که در حداقل سه تا از فیلمنامه ها جزو 75 توکن پرتکرار بودند(در اشتراک 3 مجموعه حضور داشتند) را در متغیر common_set ذخیره و چاپ میکنیم."
      ],
      "metadata": {
        "id": "3dI04i8xLZNf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "common_set=set([])\n",
        "set_eternal=set([x[0] for x in freq_analysis[\"eternal\"]])\n",
        "set_her=set([x[0] for x in freq_analysis[\"her\"]])\n",
        "set_interstellar=set([x[0] for x in freq_analysis[\"interstellar\"]])\n",
        "set_lalaland=set([x[0] for x in freq_analysis[\"lalaland\"]])\n",
        "set_mrnobody=set([x[0] for x in freq_analysis[\"mrnobody\"]])\n",
        "ommon_set=set([])\n",
        "for x in [set_eternal,set_her,set_interstellar,set_lalaland,set_mrnobody]:\n",
        "  for y in [set_eternal,set_her,set_interstellar,set_lalaland,set_mrnobody]:\n",
        "    for z in [set_eternal,set_her,set_interstellar,set_lalaland,set_mrnobody]:\n",
        "      if x!=y and x!=z and y!=z:\n",
        "        common_set=common_set.union(x&y&z)\n",
        "\n",
        "common_set\n"
      ],
      "metadata": {
        "id": "SAP8_eAlEv_r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f59a158-a462-4ff1-c062-ee29268eea90"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apartment',\n",
              " 'around',\n",
              " 'back',\n",
              " 'car',\n",
              " 'come',\n",
              " 'day',\n",
              " 'door',\n",
              " 'eye',\n",
              " 'get',\n",
              " 'going',\n",
              " 'good',\n",
              " 'hand',\n",
              " 'head',\n",
              " 'know',\n",
              " 'like',\n",
              " 'little',\n",
              " 'look',\n",
              " 'love',\n",
              " 'make',\n",
              " 'moment',\n",
              " 'night',\n",
              " 'one',\n",
              " 'open',\n",
              " 'people',\n",
              " 'pull',\n",
              " 'right',\n",
              " 'room',\n",
              " 'say',\n",
              " 'see',\n",
              " 'smile',\n",
              " 'something',\n",
              " 'start',\n",
              " 'stop',\n",
              " 'take',\n",
              " 'thing',\n",
              " 'think',\n",
              " 'time',\n",
              " 'turn',\n",
              " 'two',\n",
              " 'voice',\n",
              " 'walk',\n",
              " 'want',\n",
              " 'watch',\n",
              " 'way',\n",
              " 'well'}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "برای اینکه ببینیم فیلمنامه ها چه مفاهیم مشترکی داشتند، هرستون از دیتافریم فوق را به صورت یک مجموعه در نظر میگیریم و سپس هر توکنی که در حداقل سه تا از فیلمنامه ها جزو 50 توکن پرتکرار بودند را در متغیر common_set ذخیره و چاپ میکنیم.\n",
        "<br>\n",
        "میتوان دید در این فیلم ها مفاهیم مشترکی مانند موارد زیر وجود دارد\n",
        "<br>\n",
        "- love\n",
        "- moment\n",
        "- time\n",
        "- watch\n",
        "- look\n",
        "- eye\n",
        "- voice\n",
        "- walk\n",
        "- ..."
      ],
      "metadata": {
        "id": "rICMDx1JL07D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "<font color=\"red\" size=5>7. بررسی اثر حذف پیش پردازش ها</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "kKPdm-UrMdsu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "در این بخش توکن های خام و بدون پیش پردازش را تحلیل بسامد میکنیم و اهمیت پیش پردازش ها را مشاهده میکنیم.\n",
        "<br>\n",
        "- طبیعتا punctuationها در تمام متون از پرتکرار ترین ها هستند پس اینجا هم بهعنوان پرتکرار ها نمایششان میدهد (چون حذفشان نکردیم)\n",
        "- کلماتی مانند CONT و INT که از عبارات مخصوص فیلمنامه هستند و حذف نشدند طبیعتا پرتکرار هستند و آن ها را میبینیم.\n",
        "- کلماتی مثل THEODORE و Theodore دوبار شمرده شده اند\n",
        "- ضمایر جزو عبارات پرتکرار میشوند \n"
      ],
      "metadata": {
        "id": "JJ2mhL98Miir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sents_tocken_eternal=list(itertools.chain(*sents_tocken_eternal))\n",
        "sents_tocken_her=list(itertools.chain(*sents_tocken_her))\n",
        "sents_tocken_interstellar=list(itertools.chain(*sents_tocken_interstellar))\n",
        "sents_tocken_lalaland=list(itertools.chain(*sents_tocken_lalaland))\n",
        "sents_tocken_mrnobody=list(itertools.chain(*sents_tocken_mrnobody))\n"
      ],
      "metadata": {
        "id": "uipBwv3DEwFF"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "dataframe = {}\n",
        "\n",
        "for opt in ['eternal','her','interstellar','lalaland','mrnobody']:\n",
        "      \n",
        "     dataframe[opt] = FreqDist(eval(F\"sents_tocken_{opt}\")).most_common(50)\n",
        "\n",
        "freq_analysis = pd.DataFrame(dataframe)  \n",
        "\n"
      ],
      "metadata": {
        "id": "l4pU8GbPEwIB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "freq_analysis "
      ],
      "metadata": {
        "id": "ngd-3fxKEwKp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "41df3bc8-dc01-49cf-8cd5-5988231618c4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              eternal               her       interstellar          lalaland  \\\n",
              "0           (., 3225)         (., 2039)          (., 2510)         (., 2083)   \n",
              "1           (', 1481)         (,, 1000)          (,, 1261)         (-, 1158)   \n",
              "2           (,, 1194)          (I, 747)        (the, 1104)          (,, 809)   \n",
              "3            (I, 971)        (you, 479)          (-, 1091)        (the, 658)   \n",
              "4          (the, 795)   (THEODORE, 462)          (to, 553)          (a, 440)   \n",
              "5           (to, 556)          ((, 439)         (..., 506)        (..., 386)   \n",
              "6            (a, 506)          (), 439)      (Cooper, 494)         (to, 376)   \n",
              "7            ((, 493)         (to, 407)      (COOPER, 432)        (and, 342)   \n",
              "8            (), 493)        (the, 406)           (a, 395)          (s, 337)   \n",
              "9            (s, 474)          (a, 395)           (s, 333)        (Mia, 317)   \n",
              "10        (JOEL, 458)          (s, 393)           (?, 318)          (I, 302)   \n",
              "11         (and, 457)          (?, 332)           (I, 317)        (MIA, 260)   \n",
              "12        (Joel, 451)        (and, 320)           ((, 285)  (Sebastian, 255)   \n",
              "13         (you, 408)   (SAMANTHA, 300)           (), 285)  (SEBASTIAN, 239)   \n",
              "14           (-, 403)   (Theodore, 236)         (you, 271)          ((, 217)   \n",
              "15           (?, 379)         (it, 217)          (of, 266)          (), 217)   \n",
              "16  (CLEMENTINE, 374)         (in, 203)          (it, 256)          (?, 216)   \n",
              "17           (t, 285)          (-, 196)       (Murph, 236)        (her, 206)   \n",
              "18          (of, 282)       (that, 195)         (INT, 229)         (in, 205)   \n",
              "19          (is, 279)         (of, 188)  (CONTINUOUS, 225)        (you, 196)   \n",
              "20          (in, 274)  (CONTINUED, 186)          (at, 222)         (it, 186)   \n",
              "21          (it, 272)          (t, 175)          (is, 218)         (of, 164)   \n",
              "22  (Clementine, 216)         (on, 169)          (in, 205)         (on, 162)   \n",
              "23   (CONTINUED, 213)          (m, 160)         (and, 201)         (at, 159)   \n",
              "24          (He, 196)         (me, 155)       (BRAND, 191)        (The, 136)   \n",
              "25        (that, 186)         (He, 140)         (The, 186)        (She, 117)   \n",
              "26          (at, 182)          (', 138)       (Brand, 186)         (is, 115)   \n",
              "27           (m, 182)        (his, 136)           (t, 181)          (A, 113)   \n",
              "28         (She, 181)         (is, 132)          (on, 166)          (t, 113)   \n",
              "29          (on, 179)       (with, 120)       (MURPH, 161)         (as, 106)   \n",
              "30         (her, 177)          (:, 114)          (up, 159)         (We, 101)   \n",
              "31           (S, 174)       (just, 112)         (EXT, 148)          (we, 99)   \n",
              "32         (The, 159)         (at, 111)         (his, 133)          (It, 98)   \n",
              "33          (It, 157)        (Her, 108)         (out, 132)        (CONT, 97)   \n",
              "34        (know, 155)         (pg, 104)          (we, 129)           (D, 97)   \n",
              "35           (:, 153)       (know, 104)       (looks, 128)        (that, 92)   \n",
              "36         (his, 153)        (her, 101)         (for, 124)         (his, 91)   \n",
              "37        (with, 151)          (re, 95)         (her, 118)          (He, 90)   \n",
              "38          (me, 149)         (was, 90)        (back, 111)          (up, 90)   \n",
              "39         (You, 134)         (are, 86)        (MANN, 101)         (for, 87)   \n",
              "40         (..., 129)         (You, 85)           (DR, 99)       (looks, 85)   \n",
              "41         (don, 128)       (about, 84)            (S, 98)    (Revision, 84)   \n",
              "42         (out, 127)         (not, 84)           (as, 96)        (then, 83)   \n",
              "43         (INT, 127)        (your, 84)         (that, 94)           (:, 80)   \n",
              "44           (D, 120)         (but, 80)         (over, 94)         (INT, 80)   \n",
              "45        (CONT, 119)          (he, 78)         (into, 94)         (You, 80)   \n",
              "46          (up, 116)         (She, 78)           (re, 93)         (out, 77)   \n",
              "47       (looks, 112)         (don, 78)      (COCKPIT, 90)          (re, 76)   \n",
              "48          (my, 106)        (like, 77)           (Dr, 90)         (she, 70)   \n",
              "49        (like, 104)         (out, 77)         (this, 89)        (this, 66)   \n",
              "\n",
              "          mrnobody  \n",
              "0        (., 3895)  \n",
              "1      (the, 1513)  \n",
              "2        (,, 1220)  \n",
              "3         (-, 801)  \n",
              "4         (a, 770)  \n",
              "5      (Nemo, 714)  \n",
              "6        (is, 627)  \n",
              "7        (of, 529)  \n",
              "8        (He, 493)  \n",
              "9        (in, 489)  \n",
              "10       (to, 481)  \n",
              "11      (The, 465)  \n",
              "12      (and, 441)  \n",
              "13      (..., 433)  \n",
              "14     (NEMO, 417)  \n",
              "15      (his, 412)  \n",
              "16        (I, 383)  \n",
              "17        (s, 366)  \n",
              "18       (on, 305)  \n",
              "19        ((, 288)  \n",
              "20        (), 288)  \n",
              "21        (?, 280)  \n",
              "22      (you, 269)  \n",
              "23        (\", 248)  \n",
              "24       (at, 243)  \n",
              "25        (t, 213)  \n",
              "26      (DAY, 209)  \n",
              "27      (him, 207)  \n",
              "28       (it, 204)  \n",
              "29        (A, 187)  \n",
              "30      (her, 185)  \n",
              "31      (are, 181)  \n",
              "32      (INT, 174)  \n",
              "33     (Anna, 160)  \n",
              "34       (We, 153)  \n",
              "35      (She, 149)  \n",
              "36     (with, 148)  \n",
              "37        (S, 148)  \n",
              "38       (It, 145)  \n",
              "39       (Mr, 132)  \n",
              "40    (looks, 131)  \n",
              "41       (11, 127)  \n",
              "42   (NOBODY, 126)  \n",
              "43      (EXT, 125)  \n",
              "44  (VERSION, 124)  \n",
              "45     (They, 124)  \n",
              "46       (up, 119)  \n",
              "47      (out, 117)  \n",
              "48        (:, 116)  \n",
              "49     (CONT, 113)  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a396b328-bf10-4032-b176-16bc3fa3b3f2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>eternal</th>\n",
              "      <th>her</th>\n",
              "      <th>interstellar</th>\n",
              "      <th>lalaland</th>\n",
              "      <th>mrnobody</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>(., 3225)</td>\n",
              "      <td>(., 2039)</td>\n",
              "      <td>(., 2510)</td>\n",
              "      <td>(., 2083)</td>\n",
              "      <td>(., 3895)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>(', 1481)</td>\n",
              "      <td>(,, 1000)</td>\n",
              "      <td>(,, 1261)</td>\n",
              "      <td>(-, 1158)</td>\n",
              "      <td>(the, 1513)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>(,, 1194)</td>\n",
              "      <td>(I, 747)</td>\n",
              "      <td>(the, 1104)</td>\n",
              "      <td>(,, 809)</td>\n",
              "      <td>(,, 1220)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>(I, 971)</td>\n",
              "      <td>(you, 479)</td>\n",
              "      <td>(-, 1091)</td>\n",
              "      <td>(the, 658)</td>\n",
              "      <td>(-, 801)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>(the, 795)</td>\n",
              "      <td>(THEODORE, 462)</td>\n",
              "      <td>(to, 553)</td>\n",
              "      <td>(a, 440)</td>\n",
              "      <td>(a, 770)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>(to, 556)</td>\n",
              "      <td>((, 439)</td>\n",
              "      <td>(..., 506)</td>\n",
              "      <td>(..., 386)</td>\n",
              "      <td>(Nemo, 714)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>(a, 506)</td>\n",
              "      <td>(), 439)</td>\n",
              "      <td>(Cooper, 494)</td>\n",
              "      <td>(to, 376)</td>\n",
              "      <td>(is, 627)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>((, 493)</td>\n",
              "      <td>(to, 407)</td>\n",
              "      <td>(COOPER, 432)</td>\n",
              "      <td>(and, 342)</td>\n",
              "      <td>(of, 529)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>(), 493)</td>\n",
              "      <td>(the, 406)</td>\n",
              "      <td>(a, 395)</td>\n",
              "      <td>(s, 337)</td>\n",
              "      <td>(He, 493)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>(s, 474)</td>\n",
              "      <td>(a, 395)</td>\n",
              "      <td>(s, 333)</td>\n",
              "      <td>(Mia, 317)</td>\n",
              "      <td>(in, 489)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>(JOEL, 458)</td>\n",
              "      <td>(s, 393)</td>\n",
              "      <td>(?, 318)</td>\n",
              "      <td>(I, 302)</td>\n",
              "      <td>(to, 481)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>(and, 457)</td>\n",
              "      <td>(?, 332)</td>\n",
              "      <td>(I, 317)</td>\n",
              "      <td>(MIA, 260)</td>\n",
              "      <td>(The, 465)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>(Joel, 451)</td>\n",
              "      <td>(and, 320)</td>\n",
              "      <td>((, 285)</td>\n",
              "      <td>(Sebastian, 255)</td>\n",
              "      <td>(and, 441)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>(you, 408)</td>\n",
              "      <td>(SAMANTHA, 300)</td>\n",
              "      <td>(), 285)</td>\n",
              "      <td>(SEBASTIAN, 239)</td>\n",
              "      <td>(..., 433)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>(-, 403)</td>\n",
              "      <td>(Theodore, 236)</td>\n",
              "      <td>(you, 271)</td>\n",
              "      <td>((, 217)</td>\n",
              "      <td>(NEMO, 417)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>(?, 379)</td>\n",
              "      <td>(it, 217)</td>\n",
              "      <td>(of, 266)</td>\n",
              "      <td>(), 217)</td>\n",
              "      <td>(his, 412)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>(CLEMENTINE, 374)</td>\n",
              "      <td>(in, 203)</td>\n",
              "      <td>(it, 256)</td>\n",
              "      <td>(?, 216)</td>\n",
              "      <td>(I, 383)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>(t, 285)</td>\n",
              "      <td>(-, 196)</td>\n",
              "      <td>(Murph, 236)</td>\n",
              "      <td>(her, 206)</td>\n",
              "      <td>(s, 366)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>(of, 282)</td>\n",
              "      <td>(that, 195)</td>\n",
              "      <td>(INT, 229)</td>\n",
              "      <td>(in, 205)</td>\n",
              "      <td>(on, 305)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>(is, 279)</td>\n",
              "      <td>(of, 188)</td>\n",
              "      <td>(CONTINUOUS, 225)</td>\n",
              "      <td>(you, 196)</td>\n",
              "      <td>((, 288)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>(in, 274)</td>\n",
              "      <td>(CONTINUED, 186)</td>\n",
              "      <td>(at, 222)</td>\n",
              "      <td>(it, 186)</td>\n",
              "      <td>(), 288)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>(it, 272)</td>\n",
              "      <td>(t, 175)</td>\n",
              "      <td>(is, 218)</td>\n",
              "      <td>(of, 164)</td>\n",
              "      <td>(?, 280)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>(Clementine, 216)</td>\n",
              "      <td>(on, 169)</td>\n",
              "      <td>(in, 205)</td>\n",
              "      <td>(on, 162)</td>\n",
              "      <td>(you, 269)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>(CONTINUED, 213)</td>\n",
              "      <td>(m, 160)</td>\n",
              "      <td>(and, 201)</td>\n",
              "      <td>(at, 159)</td>\n",
              "      <td>(\", 248)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>(He, 196)</td>\n",
              "      <td>(me, 155)</td>\n",
              "      <td>(BRAND, 191)</td>\n",
              "      <td>(The, 136)</td>\n",
              "      <td>(at, 243)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>(that, 186)</td>\n",
              "      <td>(He, 140)</td>\n",
              "      <td>(The, 186)</td>\n",
              "      <td>(She, 117)</td>\n",
              "      <td>(t, 213)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>(at, 182)</td>\n",
              "      <td>(', 138)</td>\n",
              "      <td>(Brand, 186)</td>\n",
              "      <td>(is, 115)</td>\n",
              "      <td>(DAY, 209)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>(m, 182)</td>\n",
              "      <td>(his, 136)</td>\n",
              "      <td>(t, 181)</td>\n",
              "      <td>(A, 113)</td>\n",
              "      <td>(him, 207)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>(She, 181)</td>\n",
              "      <td>(is, 132)</td>\n",
              "      <td>(on, 166)</td>\n",
              "      <td>(t, 113)</td>\n",
              "      <td>(it, 204)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>(on, 179)</td>\n",
              "      <td>(with, 120)</td>\n",
              "      <td>(MURPH, 161)</td>\n",
              "      <td>(as, 106)</td>\n",
              "      <td>(A, 187)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>(her, 177)</td>\n",
              "      <td>(:, 114)</td>\n",
              "      <td>(up, 159)</td>\n",
              "      <td>(We, 101)</td>\n",
              "      <td>(her, 185)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>(S, 174)</td>\n",
              "      <td>(just, 112)</td>\n",
              "      <td>(EXT, 148)</td>\n",
              "      <td>(we, 99)</td>\n",
              "      <td>(are, 181)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>(The, 159)</td>\n",
              "      <td>(at, 111)</td>\n",
              "      <td>(his, 133)</td>\n",
              "      <td>(It, 98)</td>\n",
              "      <td>(INT, 174)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>(It, 157)</td>\n",
              "      <td>(Her, 108)</td>\n",
              "      <td>(out, 132)</td>\n",
              "      <td>(CONT, 97)</td>\n",
              "      <td>(Anna, 160)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>(know, 155)</td>\n",
              "      <td>(pg, 104)</td>\n",
              "      <td>(we, 129)</td>\n",
              "      <td>(D, 97)</td>\n",
              "      <td>(We, 153)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>(:, 153)</td>\n",
              "      <td>(know, 104)</td>\n",
              "      <td>(looks, 128)</td>\n",
              "      <td>(that, 92)</td>\n",
              "      <td>(She, 149)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>(his, 153)</td>\n",
              "      <td>(her, 101)</td>\n",
              "      <td>(for, 124)</td>\n",
              "      <td>(his, 91)</td>\n",
              "      <td>(with, 148)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>(with, 151)</td>\n",
              "      <td>(re, 95)</td>\n",
              "      <td>(her, 118)</td>\n",
              "      <td>(He, 90)</td>\n",
              "      <td>(S, 148)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>(me, 149)</td>\n",
              "      <td>(was, 90)</td>\n",
              "      <td>(back, 111)</td>\n",
              "      <td>(up, 90)</td>\n",
              "      <td>(It, 145)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>(You, 134)</td>\n",
              "      <td>(are, 86)</td>\n",
              "      <td>(MANN, 101)</td>\n",
              "      <td>(for, 87)</td>\n",
              "      <td>(Mr, 132)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>(..., 129)</td>\n",
              "      <td>(You, 85)</td>\n",
              "      <td>(DR, 99)</td>\n",
              "      <td>(looks, 85)</td>\n",
              "      <td>(looks, 131)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>(don, 128)</td>\n",
              "      <td>(about, 84)</td>\n",
              "      <td>(S, 98)</td>\n",
              "      <td>(Revision, 84)</td>\n",
              "      <td>(11, 127)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>(out, 127)</td>\n",
              "      <td>(not, 84)</td>\n",
              "      <td>(as, 96)</td>\n",
              "      <td>(then, 83)</td>\n",
              "      <td>(NOBODY, 126)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>(INT, 127)</td>\n",
              "      <td>(your, 84)</td>\n",
              "      <td>(that, 94)</td>\n",
              "      <td>(:, 80)</td>\n",
              "      <td>(EXT, 125)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>(D, 120)</td>\n",
              "      <td>(but, 80)</td>\n",
              "      <td>(over, 94)</td>\n",
              "      <td>(INT, 80)</td>\n",
              "      <td>(VERSION, 124)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>(CONT, 119)</td>\n",
              "      <td>(he, 78)</td>\n",
              "      <td>(into, 94)</td>\n",
              "      <td>(You, 80)</td>\n",
              "      <td>(They, 124)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>(up, 116)</td>\n",
              "      <td>(She, 78)</td>\n",
              "      <td>(re, 93)</td>\n",
              "      <td>(out, 77)</td>\n",
              "      <td>(up, 119)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>(looks, 112)</td>\n",
              "      <td>(don, 78)</td>\n",
              "      <td>(COCKPIT, 90)</td>\n",
              "      <td>(re, 76)</td>\n",
              "      <td>(out, 117)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>(my, 106)</td>\n",
              "      <td>(like, 77)</td>\n",
              "      <td>(Dr, 90)</td>\n",
              "      <td>(she, 70)</td>\n",
              "      <td>(:, 116)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>(like, 104)</td>\n",
              "      <td>(out, 77)</td>\n",
              "      <td>(this, 89)</td>\n",
              "      <td>(this, 66)</td>\n",
              "      <td>(CONT, 113)</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a396b328-bf10-4032-b176-16bc3fa3b3f2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a396b328-bf10-4032-b176-16bc3fa3b3f2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a396b328-bf10-4032-b176-16bc3fa3b3f2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "common_set=set([])\n",
        "set_eternal=set([x[0] for x in freq_analysis[\"eternal\"]])\n",
        "set_her=set([x[0] for x in freq_analysis[\"her\"]])\n",
        "set_interstellar=set([x[0] for x in freq_analysis[\"interstellar\"]])\n",
        "set_lalaland=set([x[0] for x in freq_analysis[\"lalaland\"]])\n",
        "set_mrnobody=set([x[0] for x in freq_analysis[\"mrnobody\"]])\n",
        "ommon_set=set([])\n",
        "for x in [set_eternal,set_her,set_interstellar,set_lalaland,set_mrnobody]:\n",
        "  for y in [set_eternal,set_her,set_interstellar,set_lalaland,set_mrnobody]:\n",
        "    for z in [set_eternal,set_her,set_interstellar,set_lalaland,set_mrnobody]:\n",
        "      if x!=y and x!=z and y!=z:\n",
        "        common_set=common_set.union(x&y&z)\n",
        "\n",
        "common_set\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsV4u7JZ9Jjk",
        "outputId": "455a5fa3-b9f2-4c81-87a5-aa90c50cf1e3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '...',\n",
              " ':',\n",
              " '?',\n",
              " 'CONT',\n",
              " 'He',\n",
              " 'I',\n",
              " 'INT',\n",
              " 'It',\n",
              " 'S',\n",
              " 'She',\n",
              " 'The',\n",
              " 'You',\n",
              " 'a',\n",
              " 'and',\n",
              " 'at',\n",
              " 'her',\n",
              " 'his',\n",
              " 'in',\n",
              " 'is',\n",
              " 'it',\n",
              " 'looks',\n",
              " 'of',\n",
              " 'on',\n",
              " 'out',\n",
              " 're',\n",
              " 's',\n",
              " 't',\n",
              " 'that',\n",
              " 'the',\n",
              " 'to',\n",
              " 'up',\n",
              " 'with',\n",
              " 'you'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"XB Zar\" size=4><div dir=rtl>\n",
        "روی هم رفته میبینیم که هیچ مفهوم مشترکی که اطلاعات جدیدی به ما اضافه کند استخراج نشد و پرتکرار های نشترک تعدادی علامت نگارشی یا ضمیر و ... هستند."
      ],
      "metadata": {
        "id": "GORCCjxhNUYC"
      }
    }
  ]
}