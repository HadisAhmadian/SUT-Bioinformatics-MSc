{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtcjhtpLiBkn"
      },
      "source": [
        "## Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfexUA5BiBkq"
      },
      "source": [
        "## Token classification on n2c2 track2 using Logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "upuBWRoSiBkm"
      },
      "outputs": [],
      "source": [
        "def train_logisticRegression(X_train, y_train):\n",
        "    param_grid = {'penalty': ['l1','l2']}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rOg94xMoiBkr"
      },
      "source": [
        "<div class=\"alert alert-warning\">\n",
        "    <strong>Warning:</strong> Run it on a subset of data for time limitaion!\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "2Il1DieNiBks",
        "outputId": "13a4fa1b-82a4-441a-b71f-3477dd69660d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-ea1c1a0a17a8>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhmmlearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhmm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Disable warning messages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hmmlearn'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import warnings\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from hmmlearn import hmm\n",
        "\n",
        "# Disable warning messages\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Directory path\n",
        "directory_path = \"/Users/sinaabdous/SinaDocuments/UniStudies/nlp/exercise/4/n2c2/2/data/test2\"\n",
        "\n",
        "# Get all CSV files in the directory\n",
        "csv_files = glob.glob(directory_path + \"/*.csv\")\n",
        "\n",
        "# Initialize empty lists to store train and test data\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "# Loop through each CSV file\n",
        "for file in csv_files:\n",
        "    # Load the CSV file\n",
        "    df = pd.read_csv(file, header=None)\n",
        "\n",
        "    # Rename the columns for clarity\n",
        "    df.columns = ['DocID', 'SentenceID', 'Word', 'Label']\n",
        "\n",
        "    # Replace missing values with an empty string\n",
        "    df['Word'] = df['Word'].fillna('')\n",
        "\n",
        "    # Split the data into train and test sets\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Append train data to the train_data list\n",
        "    train_data.append(train_df)\n",
        "\n",
        "    # Append test data to the test_data list\n",
        "    test_data.append(test_df)\n",
        "\n",
        "# Combine all train data\n",
        "train_data_combined = pd.concat(train_data, ignore_index=True)\n",
        "\n",
        "# Create the feature matrix X_train and target variable y_train\n",
        "X_train = train_data_combined['Word']\n",
        "y_train = train_data_combined['Label']\n",
        "\n",
        "# Create a CountVectorizer to convert words into numerical features\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "\n",
        "# Convert sparse matrix to dense numpy array\n",
        "X_train_vectorized = X_train_vectorized.toarray()\n",
        "\n",
        "# Initialize and train the HMM model\n",
        "model = hmm.MultinomialHMM(n_components=2)  # 2 states for simplicity\n",
        "\n",
        "# Set a valid initial state probability distribution\n",
        "startprob_prior = np.array([0.5, 0.5])  # Example: equal initial probabilities for two states\n",
        "model.startprob_ = startprob_prior\n",
        "\n",
        "# Set a valid emission probability distribution\n",
        "emissionprob_prior = np.ones((2, X_train_vectorized.shape[1]))  # Example: uniform emission probabilities\n",
        "model.emissionprob_ = emissionprob_prior\n",
        "\n",
        "# Set a valid transition probability distribution\n",
        "transition_prior = np.array([[0.5, 0.5], [0.5, 0.5]])  # Example: equal transition probabilities\n",
        "model.transmat_ = transition_prior\n",
        "\n",
        "# Fit the HMM model\n",
        "model.fit(X_train_vectorized)\n",
        "\n",
        "# Loop through each test data and perform evaluation\n",
        "for i, test_df in enumerate(test_data):\n",
        "    # Create the feature matrix X_test and target variable y_test\n",
        "    X_test = test_df['Word']\n",
        "    y_test = test_df['Label']\n",
        "\n",
        "    # Convert the words into numerical features\n",
        "    X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "    # Convert sparse matrix to dense numpy array\n",
        "    X_test_vectorized = X_test_vectorized.toarray()\n",
        "\n",
        "    # Predict labels\n",
        "    _, y_pred = model.decode(X_test_vectorized)\n",
        "\n",
        "    # Print accuracy and classification report\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    report = classification_report(y_test, y_pred)\n",
        "\n",
        "    print(\"Test Set\", i+1)\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP-Kdq4CiBkt"
      },
      "source": [
        "## Token classification on ncbi (using logistic regression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NAoz5YKxiBku",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb0bdbf9-66d4-456d-8483-376f88e7d01d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/486.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.16.2-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.5/268.5 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 huggingface-hub-0.16.2 multiprocess-0.70.14 xxhash-3.2.0\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post5.tar.gz (3.7 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "8ef0f72de4c54a67b9c5eea0ee36f0c2",
            "35dc2a366d6547a6a71d33a7afb5aa68",
            "ffc2ca705d16444fbc2606690a41b961",
            "f04a96f736c34c7bbb6e2cc33db96ec0",
            "5768c69520aa495f8609fbc9d20c39dd",
            "bc670ec5a36a4def93cf09294192c937",
            "098e42ab3d4d4e76bd5bf4bbd14c9cac",
            "75f85682697c45419bd626fef4b2b967",
            "0e2f9d4693f94034b1e5a700e69707b1",
            "a4ef087b6f47425183f81eb671237ccc",
            "e36fb5eba8be47399349fe2b0f84220e",
            "cda269dd1d0849bf83bbddc817890e63",
            "bc46f6cfe23a4cc08cda54d2b23b32cf",
            "6bfb1afb7fef4498abf7863d365978cb",
            "96276951b1964ac0a7167f95fd5f4011",
            "8695487905954d10b4f8c0e44cb0a7d2",
            "a195014d78d64bf9974f22b4c2dcea2f",
            "65613292f23e4c02b6bb18fcbfc5fd37",
            "1e1216ef7a424acda7f8fd1b8f8ed4a0",
            "30919a574e9240bc828676caa07d9f09",
            "69e238b3bf024892991542eb4a781f87",
            "997f88442fba436d87a9f5198f3efe40",
            "f3dc992deced4c0691e3462445da6cbd",
            "c0357fa86b9d4edd9eb1f85688b1de36",
            "8bb348554e994be586ae1014b1e6a703",
            "252172ba18a44177bd6c77d2f433b950",
            "daa43979be9d421b8bb3d04713c2ccb8",
            "89dcda13f3244563b8ac51d6015e56ea",
            "8b4870c830524368a1289695aec20b47",
            "d586909da14543939751ea95f369026e",
            "4805fa70e7794dec8f0e3a88c4d22d3c",
            "33cb1de4de2f4f2c8e1356ed903c931f",
            "d759c22991cf41dfb9ae8cc4e20a1062",
            "0bd1f51cea4f49a1b6f37c26e878f415",
            "e6c7484885124a3ba5b88abcff4df778",
            "a7c7b518ab8642f4bf990596d67d780d",
            "fa75e584a1ca4fc788b89814bdfeee1e",
            "49d57a3951d247f986d9d217ce3a2554",
            "3da31c89c0954c3d892f6274b67b37ca",
            "8a98a9f8e8084e2f945cefc2759af395",
            "8c477d6f28644001b2c25f5176be2642",
            "3b88b62f3e7d453ca2e926189f787df8",
            "ba06a4595f3548fb86e6e7553ceb9c69",
            "f45d2240160e463bb36fa5605518f61f",
            "30d1341b5e9746d29bb341e638ef414c",
            "1e3c8634d45d4b1cb140e5fff2e5d946",
            "cfc6864a41b840beb37ceb904058765d",
            "815af9ddf3f147f68b09e4225d05e793",
            "cca70b174c12495db52f74954400977e",
            "6324cefb289a41c59232a54e46ccaec2",
            "fccee9b6de954542977d843581af073a",
            "e9c8d3d662b6446ca31025aacb3a233c",
            "dfdf5ecaafad4a488f82cb203b54af3d",
            "6d6da88ad8f4457c940e44fd23e8f96f",
            "a9c064f5664d42e298ba1c9e1b7f9bd9",
            "e7dfdb04347b497a892d53059de204fa",
            "fd788494d07a4e2ea7e9a14397058bbe",
            "9dc90ed867104fa4a64eb8d9cee3ff52",
            "197d9aef6b334c69aa7ef60deefe460c",
            "f139c59822e44f6fa820cc53c49bb753",
            "e698fe8750614ac0bd204d65c82bd59f",
            "cd824996a0c84b80886e07505c3fc37f",
            "91747ad8b0864ef2b1ae2e54b62222bd",
            "2512c4d351c449f99e35be918b1ea928",
            "ef2044b10af649698ad3899b1fc3b68c",
            "c27731bb7e48443bbcf14fd35220225c",
            "aa1fdf5d96114a819237b74faf262b83",
            "3307c471ea4e487c905035194c3de253",
            "015a9f58552141348dbbbc4b45eb6478",
            "7246fda3583e452d9385ff93dc2c1463",
            "ab38f4d1194d4307bfcbe19fa8478d94",
            "6d9f415c2ed646d982db40f52f5180e7",
            "9571390d9cdf4f8086a5281138497461",
            "1e166686e4334969b8d3d9ea3cfd6b99",
            "b9b8463682b9482bb9ffed6882ffe434",
            "0e82f77b52004849818047b1e6e01fcd",
            "5a48ba3145544da082d831df388ce749",
            "b27bed9f5f3347609be64ebcc81cecc2",
            "b6a7b679cc8343de933d0cf1c2d63abc",
            "fd2f18cc63244d76bf7ca049456ec1dd",
            "a724c102cdd94c918cd5487383b1821e",
            "e751ce2b170440668e5a62047cd21741",
            "6d55fa2a7a3d406ea212bcfb73c3bf13",
            "6ccd59ca59e2458393ea58020062608b",
            "a8d4a8c3ae0f46dc9ff72b7aa3f53904",
            "92f65d185b3d42d2b2831b9799013095",
            "197cb934e9c14cd598868208e992092a",
            "bdd43a6cfdd842a8b448cff2fb42515e",
            "ec1e3b3b8bcc42c3af337abc3d619d15",
            "20cfc1d435054b5ea97b49672da408e9",
            "03025add92054f6cb7f53d0b01bf36f3",
            "cc5ce6afcca3444f9c6a2c244f11929f",
            "ffcb4ba00681495b939d77b42bc02d91",
            "8a417b3824f749efaec2bb0ae2e494a6",
            "1e12646b00cd49248d2fed142f626bf2",
            "0cdba4b1a9b549e2ad3c4f52a512b836",
            "91cd8221223640b4bd3c88940458d780",
            "a99d7db3a0344a149a33fc205397a31c",
            "a547d4e5165140058a33b75cafee9de0",
            "30fa11b87ea54a66aa52d593c03f902c",
            "508670da99ff4cada92158e45ff931e1",
            "43329a2db9e14b298dbafb35f1309feb",
            "800161e7fbb840819c81f40495efbd33",
            "56039bcae22d4c2799169b74b5a12d8b",
            "9870bcf1ab4e4e8c9104d85d79e5a732",
            "967d1e6cf72e47e6979e3fa1f9d10042",
            "286ef05c6fae4c7da98e08554da9895e",
            "ee88eec852ef42de9ddab316c8c31b13",
            "124371078a5e464f88499ab48f32ed5e",
            "8202e982e7c641e4bc0eddf6c3863b7e",
            "dece41d3347b48f78c3adf1bf4d407b0",
            "967b469f60994020abdd4318e703d677",
            "1be19e3876234fa8b6962575fa72b8b8",
            "953e89af94584dc7b3d1bf49503cbc08",
            "ebcd6cc283724422b92da7c91ab11dae",
            "ee3f98da39414d85b6648a2ef248cd90",
            "2a292f4693eb43c4863f612b0c6be029",
            "d4e872f7f3f3447fa6b856007d6d38c6",
            "d6f1bb63ff0c4498aecbf99c841a7821",
            "1d8e9fae3d89459c9a74eb7e73a1875d",
            "e9f959678d7246b7b12a264adbed47f9",
            "6405edac5ce34f8e897b161cd1a71fff",
            "d98d21bb718b49418035af93571cc332",
            "48e1d68b745e4c02b512de68d6d1bff2",
            "7e545b5fbb0142d390cb21651f37f79d",
            "70e9fd5cfa81486d9762d8ec6978909d",
            "44ace93cbcd548bd99b638fa53b4476e",
            "7e14ca2762c042dda2d21b049c4c1f02",
            "6e7957726ec240e5b30e5a4c79ac4fbe",
            "124a3360a9d84198abf3f3ff6104e31c",
            "b1faa1cb1eb94280b7cf875cc5fff3ab",
            "53a5cf9414754eceafec7190bd417e0c"
          ],
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eRJJyJdaiBku",
        "outputId": "48d98379-eb76-4298-9368-7ee7ea80e299"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/5.83k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ef0f72de4c54a67b9c5eea0ee36f0c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading metadata:   0%|          | 0.00/3.45k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cda269dd1d0849bf83bbddc817890e63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/9.70k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f3dc992deced4c0691e3462445da6cbd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset ncbi_disease/ncbi_disease to /root/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bd1f51cea4f49a1b6f37c26e878f415"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/284k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30d1341b5e9746d29bb341e638ef414c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/51.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7dfdb04347b497a892d53059de204fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/52.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa1fdf5d96114a819237b74faf262b83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b27bed9f5f3347609be64ebcc81cecc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/5433 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec1e3b3b8bcc42c3af337abc3d619d15"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split:   0%|          | 0/924 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "30fa11b87ea54a66aa52d593c03f902c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split:   0%|          | 0/941 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dece41d3347b48f78c3adf1bf4d407b0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset ncbi_disease downloaded and prepared to /root/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6405edac5ce34f8e897b161cd1a71fff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.9511869498101715\n",
            "Validation Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98     22092\n",
            "           1       0.68      0.46      0.55       787\n",
            "           2       0.76      0.43      0.55      1090\n",
            "\n",
            "    accuracy                           0.95     23969\n",
            "   macro avg       0.80      0.63      0.69     23969\n",
            "weighted avg       0.94      0.95      0.94     23969\n",
            "\n",
            "\n",
            "Test Accuracy: 0.9476670612728089\n",
            "Test Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.99      0.98     22450\n",
            "           1       0.77      0.49      0.60       960\n",
            "           2       0.73      0.41      0.53      1087\n",
            "\n",
            "    accuracy                           0.95     24497\n",
            "   macro avg       0.82      0.63      0.70     24497\n",
            "weighted avg       0.94      0.95      0.94     24497\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"ncbi_disease\")\n",
        "\n",
        "# Extract the train, validation, and test sets\n",
        "train_data = dataset[\"train\"]\n",
        "valid_data = dataset[\"validation\"]\n",
        "test_data = dataset[\"test\"]\n",
        "\n",
        "# Preprocess the data and convert it into feature and target vectors\n",
        "def preprocess_data(data):\n",
        "    X = []\n",
        "    y = []\n",
        "    for instance in data:\n",
        "        tokens = instance[\"tokens\"]\n",
        "        labels = instance[\"ner_tags\"]\n",
        "        for token, label in zip(tokens, labels):\n",
        "            X.append(token)\n",
        "            y.append(label)\n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = preprocess_data(train_data)\n",
        "X_valid, y_valid = preprocess_data(valid_data)\n",
        "X_test, y_test = preprocess_data(test_data)\n",
        "\n",
        "# Create a CountVectorizer to convert the tokenized text into numerical features\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_valid_vectorized = vectorizer.transform(X_valid)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "# Predict the labels for the validation and test sets\n",
        "y_valid_pred = model.predict(X_valid_vectorized)\n",
        "y_test_pred = model.predict(X_test_vectorized)\n",
        "\n",
        "# Print accuracy and classification reports\n",
        "valid_accuracy = accuracy_score(y_valid, y_valid_pred)\n",
        "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "valid_report = classification_report(y_valid, y_valid_pred)\n",
        "test_report = classification_report(y_test, y_test_pred)\n",
        "\n",
        "print(\"Validation Accuracy:\", valid_accuracy)\n",
        "print(\"Validation Report:\")\n",
        "print(valid_report)\n",
        "\n",
        "print(\"\\nTest Accuracy:\", test_accuracy)\n",
        "print(\"Test Report:\")\n",
        "print(test_report)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gn2Dw9doiBkw"
      },
      "source": [
        "## Token Classification on n2c2 track2 data using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBQmvLoKiBkx"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "\n",
        "directory_path = \"/Users/sinaabdous/SinaDocuments/UniStudies/nlp/exercise/4/n2c2/2/data/test\"\n",
        "\n",
        "\n",
        "csv_files = glob.glob(directory_path + \"/*.csv\")\n",
        "\n",
        "\n",
        "train_data = []\n",
        "test_data = []\n",
        "\n",
        "\n",
        "for file in csv_files:\n",
        "    df = pd.read_csv(file, header=None)\n",
        "    df.columns = ['SentenceID', 'WordID', 'Word', 'Label']\n",
        "    df['Word'] = df['Word'].fillna('')\n",
        "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "    train_data.append(train_df)\n",
        "    test_data.append(test_df)\n",
        "\n",
        "\n",
        "train_data_combined = pd.concat(train_data, ignore_index=True)\n",
        "X_train = train_data_combined['Word']\n",
        "y_train = train_data_combined['Label']\n",
        "\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "\n",
        "\n",
        "\n",
        "max_sequence_length = max(len(seq) for seq in X_train_sequences)\n",
        "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length)\n",
        "\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "\n",
        "embedding_dim = 100\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=embedding_dim, input_length=max_sequence_length))\n",
        "model.add(LSTM(units=100, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(units=num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "model.fit(X_train_padded, y_train_encoded, epochs=2, batch_size=32)\n",
        "\n",
        "\n",
        "test_data_combined = pd.concat(test_data, ignore_index=True)\n",
        "X_test = test_data_combined['Word']\n",
        "y_test = test_data_combined['Label']\n",
        "\n",
        "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "y_test_pred = model.predict_classes(X_test_padded)\n",
        "\n",
        "accuracy = accuracy_score(y_test_encoded, y_test_pred)\n",
        "report = classification_report(y_test_encoded, y_test_pred)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icgRmcEZiBky"
      },
      "source": [
        "## Token classification on ncbi using LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "ca03395c341e48369ffb44032d791e18"
          ]
        },
        "id": "ZbimQYPmiBky",
        "outputId": "5f22deab-ff14-4eca-9f92-474b7b8c3ee6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Found cached dataset ncbi_disease (/Users/sinaabdous/.cache/huggingface/datasets/ncbi_disease/ncbi_disease/1.0.0/92314c7992b0b8a5ea2ad101be33f365b684a2cc011e0ffa29c691e6d32b2d03)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca03395c341e48369ffb44032d791e18",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "170/170 [==============================] - 41s 230ms/step - loss: 0.1727 - accuracy: 0.9564 - val_loss: 0.0626 - val_accuracy: 0.9830\n",
            "Epoch 2/10\n",
            "170/170 [==============================] - 38s 226ms/step - loss: 0.0376 - accuracy: 0.9886 - val_loss: 0.0312 - val_accuracy: 0.9910\n",
            "Epoch 3/10\n",
            "170/170 [==============================] - 40s 236ms/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 0.0316 - val_accuracy: 0.9915\n",
            "Epoch 4/10\n",
            "170/170 [==============================] - 38s 222ms/step - loss: 0.0157 - accuracy: 0.9948 - val_loss: 0.0280 - val_accuracy: 0.9915\n",
            "Epoch 5/10\n",
            "170/170 [==============================] - 38s 226ms/step - loss: 0.0126 - accuracy: 0.9957 - val_loss: 0.0279 - val_accuracy: 0.9921\n",
            "Epoch 6/10\n",
            "170/170 [==============================] - 40s 235ms/step - loss: 0.0104 - accuracy: 0.9966 - val_loss: 0.0301 - val_accuracy: 0.9925\n",
            "Epoch 7/10\n",
            "170/170 [==============================] - 38s 225ms/step - loss: 0.0084 - accuracy: 0.9972 - val_loss: 0.0277 - val_accuracy: 0.9924\n",
            "Epoch 8/10\n",
            "170/170 [==============================] - 41s 241ms/step - loss: 0.0068 - accuracy: 0.9978 - val_loss: 0.0311 - val_accuracy: 0.9921\n",
            "Epoch 9/10\n",
            "170/170 [==============================] - 39s 228ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0324 - val_accuracy: 0.9921\n",
            "Epoch 10/10\n",
            "170/170 [==============================] - 39s 228ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0361 - val_accuracy: 0.9915\n",
            "30/30 [==============================] - 2s 76ms/step - loss: 0.0406 - accuracy: 0.9908\n",
            "Test accuracy: 0.990841805934906\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, TimeDistributed, Dense\n",
        "\n",
        "# Load dataset\n",
        "dataset = load_dataset(\"ncbi_disease\")\n",
        "train_data = dataset[\"train\"]\n",
        "val_data = dataset[\"validation\"]\n",
        "test_data = dataset[\"test\"]\n",
        "\n",
        "# Tokenize words and tags\n",
        "word_tokenizer = Tokenizer(filters='', lower=False, oov_token='<UNK>')\n",
        "tag_tokenizer = Tokenizer(filters='', lower=False)\n",
        "\n",
        "word_tokenizer.fit_on_texts(train_data[\"tokens\"])\n",
        "tag_tokenizer.fit_on_texts(train_data[\"ner_tags\"])\n",
        "\n",
        "# Convert words and tags to sequences\n",
        "X_train = word_tokenizer.texts_to_sequences(train_data[\"tokens\"])\n",
        "y_train = tag_tokenizer.texts_to_sequences(train_data[\"ner_tags\"])\n",
        "X_val = word_tokenizer.texts_to_sequences(val_data[\"tokens\"])\n",
        "y_val = tag_tokenizer.texts_to_sequences(val_data[\"ner_tags\"])\n",
        "X_test = word_tokenizer.texts_to_sequences(test_data[\"tokens\"])\n",
        "y_test = tag_tokenizer.texts_to_sequences(test_data[\"ner_tags\"])\n",
        "\n",
        "# Pad sequences\n",
        "max_seq_len = max([len(seq) for seq in X_train])  # You can also set an arbitrary number\n",
        "X_train = pad_sequences(X_train, maxlen=max_seq_len, padding='post')\n",
        "y_train = pad_sequences(y_train, maxlen=max_seq_len, padding='post')\n",
        "X_val = pad_sequences(X_val, maxlen=max_seq_len, padding='post')\n",
        "y_val = pad_sequences(y_val, maxlen=max_seq_len, padding='post')\n",
        "X_test = pad_sequences(X_test, maxlen=max_seq_len, padding='post')\n",
        "y_test = pad_sequences(y_test, maxlen=max_seq_len, padding='post')\n",
        "\n",
        "# One-hot encode labels\n",
        "y_train = to_categorical(y_train)\n",
        "y_val = to_categorical(y_val)\n",
        "y_test = to_categorical(y_test)\n",
        "\n",
        "# Build LSTM model\n",
        "vocab_size = len(word_tokenizer.word_index) + 1\n",
        "num_tags = len(tag_tokenizer.word_index) + 1\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_seq_len),\n",
        "    LSTM(units=256, return_sequences=True),\n",
        "    TimeDistributed(Dense(units=num_tags, activation='softmax'))\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), batch_size=32, epochs=10)\n",
        "\n",
        "# Evaluate model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, batch_size=32)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ]
    }
  ]
}